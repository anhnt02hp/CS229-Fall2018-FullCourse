{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "# PS4-6: Reinforcement Learning: The inverted pendulum",
   "id": "e957a34f540bccb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Library",
   "id": "65bb05c2f801c4a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T07:13:44.523904Z",
     "start_time": "2025-08-31T07:13:44.517996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import division, print_function\n",
    "from src.env import CartPole, Physics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import lfilter"
   ],
   "id": "41c9d2cd2fefb3a0",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Global Parameters",
   "id": "c4adfc2a235e81ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T07:13:44.552653Z",
     "start_time": "2025-08-31T07:13:44.548102Z"
    }
   },
   "cell_type": "code",
   "source": "gamma = 0.995",
   "id": "2bd68d4ba5f3a4ef",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training RL Model",
   "id": "5d736ead2ccda755"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training Algorithm",
   "id": "52c1b4ee6c8c9cca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T07:13:44.609221Z",
     "start_time": "2025-08-31T07:13:44.600647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_mdp_data(num_states):\n",
    "    \"\"\"\n",
    "    Return a variable that contains all the parameters/state you need for your MDP.\n",
    "    Feel free to use whatever data type is most convenient for you (custom classes, tuples, dicts, etc)\n",
    "\n",
    "    Assume that no transitions or rewards have been observed.\n",
    "    Initialize the value function array to small random values (0 to 0.10, say).\n",
    "    Initialize the transition probabilities uniformly (ie, probability of\n",
    "        transitioning for state x to state y using action a is exactly\n",
    "        1/num_states).\n",
    "    Initialize all state rewards to zero.\n",
    "\n",
    "    Args:\n",
    "        num_states: The number of states\n",
    "\n",
    "    Returns: The initial MDP parameters\n",
    "    \"\"\"\n",
    "    transition_counts = np.zeros((num_states, num_states, 2))               # 3D array: (s, s', a)\n",
    "    transition_probs = np.ones((num_states, num_states, 2)) / num_states    # 3D array: (s, s', a)\n",
    "    # Index zero is count of rewards being -1 , index 1 is count of total num state is reached\n",
    "    reward_counts = np.zeros((num_states, 2))           # 2D array: (s, a)\n",
    "    reward = np.zeros(num_states)                       # 1D array: (s,)\n",
    "    value = np.random.rand(num_states) * 0.1            # 1D array: (s,)\n",
    "\n",
    "    return {\n",
    "        'transition_counts': transition_counts,\n",
    "        'transition_probs': transition_probs,\n",
    "        'reward_counts': reward_counts,\n",
    "        'reward': reward,\n",
    "        'value': value,\n",
    "        'num_states': num_states,\n",
    "    }"
   ],
   "id": "f5a86f4f339d7f69",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T07:13:44.663144Z",
     "start_time": "2025-08-31T07:13:44.658478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def choose_action(state, mdp_data):\n",
    "    \"\"\"\n",
    "    Choose the next action (0 or 1) that is optimal according to your current\n",
    "    mdp_data. When there is no optimal action, return a random action.\n",
    "\n",
    "    Args:\n",
    "        state: The current state in the MDP\n",
    "        mdp_data: The parameters for your MDP. See initialize_mdp_data.\n",
    "\n",
    "    Returns:\n",
    "        0 or 1 that is optimal according to your current MDP\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "\n",
    "    # 1. Initialize Current Parameters at State s\n",
    "    transition_probs = mdp_data['transition_probs']\n",
    "    value = mdp_data['value']\n",
    "    reward = mdp_data['reward']\n",
    "    num_states = mdp_data['num_states']\n",
    "    num_actions = 2         # Because we only have 2 action: 0 or 1\n",
    "\n",
    "    # 2. Calculate V(s) for each action\n",
    "    #V(s) = R(s) + gamma * ∑(s'∈S) Psa(s')*V(s')\n",
    "    action_value = np.zeros(num_actions)\n",
    "    for a in range(num_actions):\n",
    "        future_value = 0\n",
    "        for s_next in range(num_states):\n",
    "            future_value += transition_probs[state, s_next, a] * value[s_next]\n",
    "\n",
    "        action_value[a] = reward[state] + gamma * future_value\n",
    "    action_value = np.round(action_value, 4)        # Rounding to 4 values decimal\n",
    "\n",
    "    # 3. Get Optimal Value Action V*(s) & Choose Optimal Action a*\n",
    "    #V*(s) = max_(a∈A)(V(s)) => a* = argMax(a∈A)(V(s))\n",
    "    optimal_value = -1e9\n",
    "    optimal_action = 0\n",
    "    cnt = 0\n",
    "\n",
    "    for a in range(num_actions):\n",
    "        if optimal_value < action_value[a]:\n",
    "            optimal_value = action_value[a]\n",
    "            optimal_action = a\n",
    "            cnt = 1\n",
    "        elif optimal_value == action_value[a]:\n",
    "            cnt += 1\n",
    "\n",
    "    # 4. If no optimal action, return a random action\n",
    "    if cnt > 1:\n",
    "        actions = [0, 1]\n",
    "        optimal_action = np.random.choice(actions)\n",
    "\n",
    "    return optimal_action\n",
    "    # *** END CODE HERE ***"
   ],
   "id": "299a34a38aef4de9",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T07:13:44.716547Z",
     "start_time": "2025-08-31T07:13:44.711526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_mdp_transition_counts_reward_counts(mdp_data, state, action, new_state, reward):\n",
    "    \"\"\"\n",
    "    Update the transition count and reward count information in your mdp_data.\n",
    "    Do not change the other MDP parameters (those get changed later).\n",
    "\n",
    "    Record the number of times `state, action, new_state` occurs.\n",
    "    Record the rewards for every `new_state`\n",
    "    (since rewards are -1 or 0, you just need to record number of times reward -1 is seen in 'reward_counts' index new_state,0)\n",
    "    Record the number of time `new_state` was reached (in 'reward_counts' index new_state,1)\n",
    "\n",
    "    Args:\n",
    "        mdp_data: The parameters of your MDP. See initialize_mdp_data.\n",
    "        state: The state that was observed at the start.\n",
    "        action: The action you performed.\n",
    "        new_state: The state after your action.\n",
    "        reward: The reward after your action (i.e. reward corresponding to new_state).\n",
    "\n",
    "    Returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "    # This function does not return anything\n",
    "    return"
   ],
   "id": "3fce76ce348bcdcb",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T07:13:44.770347Z",
     "start_time": "2025-08-31T07:13:44.765694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_mdp_transition_probs_reward(mdp_data):\n",
    "    \"\"\"\n",
    "    Update the estimated transition probabilities and reward values in your MDP.\n",
    "\n",
    "    Make sure you account for the case when a state-action pair has never\n",
    "    been tried before, or the state has never been visited before. In that\n",
    "    case, you must not change that component (and thus keep it at the\n",
    "    initialized uniform distribution).\n",
    "\n",
    "    Args:\n",
    "        mdp_data: The data for your MDP. See initialize_mdp_data.\n",
    "\n",
    "    Returns:\n",
    "        Nothing\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "    # This function does not return anything\n",
    "    return"
   ],
   "id": "7f67507e5e00a03f",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T07:13:44.824623Z",
     "start_time": "2025-08-31T07:13:44.819708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_mdp_value(mdp_data, tolerance, gamma):\n",
    "    \"\"\"\n",
    "    Update the estimated values in your MDP.\n",
    "\n",
    "    Perform value iteration using the new estimated model for the MDP.\n",
    "    The convergence criterion should be based on `TOLERANCE` as described\n",
    "    at the top of the file.\n",
    "\n",
    "    Return true if it converges within one iteration.\n",
    "\n",
    "    Args:\n",
    "        mdp_data: The data for your MDP. See initialize_mdp_data.\n",
    "        tolerance: The tolerance to use for the convergence criterion.\n",
    "        gamma: Your discount factor.\n",
    "\n",
    "    Returns:\n",
    "        True if the value iteration converged in one iteration\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    # *** END CODE HERE ***"
   ],
   "id": "30ee6a54069e176f",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training Model",
   "id": "69eb52c98149cb17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T07:13:44.902445Z",
     "start_time": "2025-08-31T07:13:44.874039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main(plot=True):\n",
    "    # Seed the randomness of the simulation so this outputs the same thing each time\n",
    "    seed = 0\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Simulation parameters\n",
    "    pause_time = 0.0001\n",
    "    min_trial_length_to_start_display = 100\n",
    "    display_started = min_trial_length_to_start_display == 0\n",
    "\n",
    "    NUM_STATES = 163\n",
    "    GAMMA = 0.995\n",
    "    TOLERANCE = 0.01\n",
    "    NO_LEARNING_THRESHOLD = 20\n",
    "\n",
    "    # Time cycle of the simulation\n",
    "    time = 0\n",
    "\n",
    "    # These variables perform bookkeeping (how many cycles was the pole\n",
    "    # balanced for before it fell). Useful for plotting learning curves.\n",
    "    time_steps_to_failure = []\n",
    "    num_failures = 0\n",
    "    time_at_start_of_current_trial = 0\n",
    "\n",
    "    # You should reach convergence well before this\n",
    "    max_failures = 500\n",
    "\n",
    "    # Initialize a cart pole\n",
    "    cart_pole = CartPole(Physics())\n",
    "\n",
    "    # Starting `state_tuple` is (0, 0, 0, 0)\n",
    "    # x, x_dot, theta, theta_dot represents the actual continuous state vector\n",
    "    x, x_dot, theta, theta_dot = 0.0, 0.0, 0.0, 0.0\n",
    "    state_tuple = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "    # `state` is the number given to this state, you only need to consider\n",
    "    # this representation of the state\n",
    "    state = cart_pole.get_state(state_tuple)\n",
    "    # if min_trial_length_to_start_display == 0 or display_started == 1:\n",
    "    #     cart_pole.show_cart(state_tuple, pause_time)\n",
    "\n",
    "    mdp_data = initialize_mdp_data(NUM_STATES)\n",
    "\n",
    "    # This is the criterion to end the simulation.\n",
    "    # You should change it to terminate when the previous\n",
    "    # 'NO_LEARNING_THRESHOLD' consecutive value function computations all\n",
    "    # converged within one value function iteration. Intuitively, it seems\n",
    "    # like there will be little learning after this, so end the simulation\n",
    "    # here, and say the overall algorithm has converged.\n",
    "\n",
    "    consecutive_no_learning_trials = 0\n",
    "    while consecutive_no_learning_trials < NO_LEARNING_THRESHOLD:\n",
    "\n",
    "        action = choose_action(state, mdp_data)\n",
    "\n",
    "        # Get the next state by simulating the dynamics\n",
    "        state_tuple = cart_pole.simulate(action, state_tuple)\n",
    "        # x, x_dot, theta, theta_dot = state_tuple\n",
    "\n",
    "        # Increment simulation time\n",
    "        time = time + 1\n",
    "\n",
    "        # Get the state number corresponding to new state vector\n",
    "        new_state = cart_pole.get_state(state_tuple)\n",
    "        # if display_started == 1:\n",
    "        #     cart_pole.show_cart(state_tuple, pause_time)\n",
    "\n",
    "        # reward function to use - do not change this!\n",
    "        if new_state == NUM_STATES - 1:\n",
    "            R = -1\n",
    "        else:\n",
    "            R = 0\n",
    "\n",
    "        update_mdp_transition_counts_reward_counts(mdp_data, state, action, new_state, R)\n",
    "\n",
    "        # Recompute MDP model whenever pole falls\n",
    "        # Compute the value function V for the new model\n",
    "        if new_state == NUM_STATES - 1:\n",
    "\n",
    "            update_mdp_transition_probs_reward(mdp_data)\n",
    "\n",
    "            converged_in_one_iteration = update_mdp_value(mdp_data, TOLERANCE, GAMMA)\n",
    "\n",
    "            if converged_in_one_iteration:\n",
    "                consecutive_no_learning_trials = consecutive_no_learning_trials + 1\n",
    "            else:\n",
    "                consecutive_no_learning_trials = 0\n",
    "\n",
    "        # Do NOT change this code: Controls the simulation, and handles the case\n",
    "        # when the pole fell and the state must be reinitialized.\n",
    "        if new_state == NUM_STATES - 1:\n",
    "            num_failures += 1\n",
    "            if num_failures >= max_failures:\n",
    "                break\n",
    "            print('[INFO] Failure number {}'.format(num_failures))\n",
    "            time_steps_to_failure.append(time - time_at_start_of_current_trial)\n",
    "            # time_steps_to_failure[num_failures] = time - time_at_start_of_current_trial\n",
    "            time_at_start_of_current_trial = time\n",
    "\n",
    "            if time_steps_to_failure[num_failures - 1] > min_trial_length_to_start_display:\n",
    "                display_started = 1\n",
    "\n",
    "            # Reinitialize state\n",
    "            # x = 0.0\n",
    "            x = -1.1 + np.random.uniform() * 2.2\n",
    "            x_dot, theta, theta_dot = 0.0, 0.0, 0.0\n",
    "            state_tuple = (x, x_dot, theta, theta_dot)\n",
    "            state = cart_pole.get_state(state_tuple)\n",
    "        else:\n",
    "            state = new_state\n",
    "\n",
    "    if plot:\n",
    "        # plot the learning curve (time balanced vs. trial)\n",
    "        log_tstf = np.log(np.array(time_steps_to_failure))\n",
    "        plt.plot(np.arange(len(time_steps_to_failure)), log_tstf, 'k')\n",
    "        window = 30\n",
    "        w = np.array([1/window for _ in range(window)])\n",
    "        weights = lfilter(w, 1, log_tstf)\n",
    "        x = np.arange(window//2, len(log_tstf) - window//2)\n",
    "        plt.plot(x, weights[window:len(log_tstf)], 'r--')\n",
    "        plt.xlabel('Num failures')\n",
    "        plt.ylabel('Log of num steps to failure')\n",
    "        plt.title('seed = {}'.format(seed))\n",
    "        plt.savefig('output/control_{}.png'.format(seed))\n",
    "\n",
    "    return np.array(time_steps_to_failure)"
   ],
   "id": "5fada261f99dd59a",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T07:13:44.944373Z",
     "start_time": "2025-08-31T07:13:44.939999Z"
    }
   },
   "cell_type": "code",
   "source": "# main()",
   "id": "bc16cb4f7dc4cc08",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T07:13:44.997270Z",
     "start_time": "2025-08-31T07:13:44.994184Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9cebdae802d55dab",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
