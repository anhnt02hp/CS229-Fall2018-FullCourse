{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "# PS4-6: Reinforcement Learning: The inverted pendulum",
   "id": "e957a34f540bccb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Library",
   "id": "65bb05c2f801c4a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T16:47:02.564535Z",
     "start_time": "2025-08-31T16:47:02.558033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import division, print_function\n",
    "from src.env import CartPole, Physics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import lfilter"
   ],
   "id": "41c9d2cd2fefb3a0",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training RL Model",
   "id": "5d736ead2ccda755"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training Algorithm",
   "id": "52c1b4ee6c8c9cca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T16:47:02.589808Z",
     "start_time": "2025-08-31T16:47:02.580082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_mdp_data(num_states):\n",
    "    \"\"\"\n",
    "    Return a variable that contains all the parameters/state you need for your MDP.\n",
    "    Feel free to use whatever data type is most convenient for you (custom classes, tuples, dicts, etc)\n",
    "\n",
    "    Assume that no transitions or rewards have been observed.\n",
    "    Initialize the value function array to small random values (0 to 0.10, say).\n",
    "    Initialize the transition probabilities uniformly (ie, probability of\n",
    "        transitioning for state x to state y using action a is exactly\n",
    "        1/num_states).\n",
    "    Initialize all state rewards to zero.\n",
    "\n",
    "    Args:\n",
    "        num_states: The number of states\n",
    "\n",
    "    Returns: The initial MDP parameters\n",
    "    \"\"\"\n",
    "    transition_counts = np.zeros((num_states, num_states, 2))               # 3D array: (s, s', a)\n",
    "    transition_probs = np.ones((num_states, num_states, 2)) / num_states    # 3D array: (s, s', a)\n",
    "    # Index zero is count of rewards being -1 , index 1 is count of total num state is reached\n",
    "    reward_counts = np.zeros((num_states, 2))           # 2D array: (s, a)\n",
    "    reward = np.zeros(num_states)                       # 1D array: (s,)\n",
    "    value = np.random.rand(num_states) * 0.1            # 1D array: (s,)\n",
    "\n",
    "    return {\n",
    "        'transition_counts': transition_counts,\n",
    "        'transition_probs': transition_probs,\n",
    "        'reward_counts': reward_counts,\n",
    "        'reward': reward,\n",
    "        'value': value,\n",
    "        'num_states': num_states,\n",
    "    }"
   ],
   "id": "f5a86f4f339d7f69",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T16:47:02.647358Z",
     "start_time": "2025-08-31T16:47:02.635989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def choose_action(state, mdp_data):\n",
    "    \"\"\"\n",
    "    Choose the next action (0 or 1) that is optimal according to your current\n",
    "    mdp_data. When there is no optimal action, return a random action.\n",
    "\n",
    "    Args:\n",
    "        state: The current state in the MDP\n",
    "        mdp_data: The parameters for your MDP. See initialize_mdp_data.\n",
    "\n",
    "    Returns:\n",
    "        0 or 1 that is optimal according to your current MDP\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "\n",
    "    # 1. Initialize Current Parameters at State s\n",
    "    transition_probs = mdp_data['transition_probs']\n",
    "    value = mdp_data['value']\n",
    "    num_states = mdp_data['num_states']\n",
    "    num_actions = 2         # Because we only have 2 action: 0 or 1\n",
    "\n",
    "    # 2. Calculate V(s) for each action\n",
    "    #V(s) = ∑(s'∈S) Psa(s')*V(s')\n",
    "    action_value = np.zeros(num_actions)\n",
    "    for a in range(num_actions):\n",
    "        for s_next in range(num_states):\n",
    "            action_value[a] += transition_probs[state, s_next, a] * value[s_next]\n",
    "\n",
    "    # 3. Get Optimal Value Action V*(s) & Choose Optimal Action a*\n",
    "    #V*(s) = max_(a∈A)(V(s)) => a* = argMax(a∈A)(V(s))\n",
    "    optimal_value = -1e9\n",
    "    optimal_action = 0\n",
    "    cnt = 0\n",
    "\n",
    "    for a in range(num_actions):\n",
    "        if optimal_value < action_value[a]:\n",
    "            optimal_value = action_value[a]\n",
    "            optimal_action = a\n",
    "            cnt = 1\n",
    "        elif optimal_value == action_value[a]:\n",
    "            cnt += 1\n",
    "\n",
    "    # 4. If no optimal action, return a random action\n",
    "    if cnt > 1:\n",
    "        actions = [0, 1]\n",
    "        optimal_action = np.random.choice(actions)\n",
    "\n",
    "    return optimal_action\n",
    "    # *** END CODE HERE ***"
   ],
   "id": "299a34a38aef4de9",
   "outputs": [],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T16:47:02.706266Z",
     "start_time": "2025-08-31T16:47:02.695748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_mdp_transition_counts_reward_counts(mdp_data, state, action, new_state, reward):\n",
    "    \"\"\"\n",
    "    Update the transition count and reward count information in your mdp_data.\n",
    "    Do not change the other MDP parameters (those get changed later).\n",
    "\n",
    "    Record the number of times `state, action, new_state` occurs.\n",
    "    Record the rewards for every `new_state`\n",
    "    (since rewards are -1 or 0, you just need to record number of times reward -1 is seen in 'reward_counts' index new_state,0)\n",
    "    Record the number of time `new_state` was reached (in 'reward_counts' index new_state,1)\n",
    "\n",
    "    Args:\n",
    "        mdp_data: The parameters of your MDP. See initialize_mdp_data.\n",
    "        state: The state that was observed at the start.\n",
    "        action: The action you performed.\n",
    "        new_state: The state after your action.\n",
    "        reward: The reward after your action (i.e. reward corresponding to new_state).\n",
    "\n",
    "    Returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    # 1. Get All Current Parameters at State s\n",
    "    transition_counts = mdp_data['transition_counts']\n",
    "    reward_counts = mdp_data['reward_counts']\n",
    "\n",
    "    # 2. Update transition_counts\n",
    "    transition_counts[state, new_state, action] += 1\n",
    "\n",
    "    # 3. Update reward_counts\n",
    "    if reward == -1:\n",
    "        reward_counts[new_state, 0] += 1\n",
    "\n",
    "    reward_counts[new_state, 1] += 1\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "    # This function does not return anything\n",
    "    return"
   ],
   "id": "3fce76ce348bcdcb",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T16:47:02.767736Z",
     "start_time": "2025-08-31T16:47:02.755874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_mdp_transition_probs_reward(mdp_data):\n",
    "    \"\"\"\n",
    "    Update the estimated transition probabilities and reward values in your MDP.\n",
    "\n",
    "    Make sure you account for the case when a state-action pair has never\n",
    "    been tried before, or the state has never been visited before. In that\n",
    "    case, you must not change that component (and thus keep it at the\n",
    "    initialized uniform distribution).\n",
    "\n",
    "    Args:\n",
    "        mdp_data: The data for your MDP. See initialize_mdp_data.\n",
    "\n",
    "    Returns:\n",
    "        Nothing\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    # 1. Get All Current Parameters at State s\n",
    "    transition_counts = mdp_data['transition_counts']\n",
    "    transition_probs = mdp_data['transition_probs']\n",
    "    reward_counts = mdp_data['reward_counts']\n",
    "    reward = mdp_data['reward']\n",
    "    num_states = mdp_data['num_states']\n",
    "    num_actions = 2         # Because we only have 2 action: 0 or 1\n",
    "\n",
    "    # 2. Update transition_probs Psa(s') & Update reward R(s)\n",
    "    for s in range(num_states):\n",
    "        for a in range(num_actions):\n",
    "            total = 0\n",
    "            for s_next in range(num_states):\n",
    "                total += transition_counts[s, s_next, a]\n",
    "\n",
    "            if total > 0:\n",
    "                #Update reward\n",
    "                if reward_counts[s, 1] > 0:\n",
    "                    reward[s] = -1 * reward_counts[s,0] / reward_counts[s,1]\n",
    "\n",
    "                #update transition_probs\n",
    "                for s_next in range(num_states):\n",
    "                    transition_probs[s, s_next, a] = transition_counts[s, s_next, a] / total\n",
    "\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "    # This function does not return anything\n",
    "    return"
   ],
   "id": "7f67507e5e00a03f",
   "outputs": [],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T16:47:02.829627Z",
     "start_time": "2025-08-31T16:47:02.816082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_mdp_value(mdp_data, tolerance, gamma):\n",
    "    \"\"\"\n",
    "    Update the estimated values in your MDP.\n",
    "\n",
    "    Perform value iteration using the new estimated model for the MDP.\n",
    "    The convergence criterion should be based on `TOLERANCE` as described\n",
    "    at the top of the file.\n",
    "\n",
    "    Return true if it converges within one iteration.\n",
    "\n",
    "    Args:\n",
    "        mdp_data: The data for your MDP. See initialize_mdp_data.\n",
    "        tolerance: The tolerance to use for the convergence criterion.\n",
    "        gamma: Your discount factor.\n",
    "\n",
    "    Returns:\n",
    "        True if the value iteration converged in one iteration\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "\n",
    "    # 1. Get All Current Parameters in State s\n",
    "    transition_probs = mdp_data['transition_probs']\n",
    "    reward = mdp_data['reward']\n",
    "    value = mdp_data['value']\n",
    "    num_states = mdp_data['num_states']\n",
    "    num_actions = 2\n",
    "\n",
    "    # 2. Update MDP\n",
    "    old_value = value.copy()\n",
    "\n",
    "    for s in range(num_states):\n",
    "        action_values = np.zeros(num_actions)\n",
    "        for a in range(num_actions):\n",
    "            for s_next in range(num_states):\n",
    "                action_values[a] += transition_probs[s, s_next, a] * old_value[s_next]\n",
    "            action_values[a] = reward[s] + gamma * action_values[a]\n",
    "\n",
    "        # 3. Find V*\n",
    "        optimal_value = -1e9\n",
    "        for a in range(num_actions):\n",
    "            if optimal_value < action_values[a]:\n",
    "                optimal_value = action_values[a]\n",
    "        value[s] = optimal_value\n",
    "\n",
    "    # 4. Check convergence\n",
    "    converged = False\n",
    "    cnt = 0\n",
    "    for s in range(num_states):\n",
    "        if abs(value[s] - old_value[s]) < tolerance:\n",
    "            cnt += 1\n",
    "    if cnt == num_states:\n",
    "        converged = True\n",
    "\n",
    "    # 5. Save back\n",
    "    mdp_data['value'] = value\n",
    "    return converged\n",
    "    # *** END CODE HERE ***"
   ],
   "id": "30ee6a54069e176f",
   "outputs": [],
   "execution_count": 116
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training Model",
   "id": "69eb52c98149cb17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T16:47:02.903427Z",
     "start_time": "2025-08-31T16:47:02.877505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main(plot=True):\n",
    "    # Seed the randomness of the simulation so this outputs the same thing each time\n",
    "    seed = 0\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Simulation parameters\n",
    "    pause_time = 0.0001\n",
    "    min_trial_length_to_start_display = 100\n",
    "    display_started = min_trial_length_to_start_display == 0\n",
    "\n",
    "    NUM_STATES = 163\n",
    "    GAMMA = 0.995\n",
    "    TOLERANCE = 0.01\n",
    "    NO_LEARNING_THRESHOLD = 20\n",
    "\n",
    "    # Time cycle of the simulation\n",
    "    time = 0\n",
    "\n",
    "    # These variables perform bookkeeping (how many cycles was the pole\n",
    "    # balanced for before it fell). Useful for plotting learning curves.\n",
    "    time_steps_to_failure = []\n",
    "    num_failures = 0\n",
    "    time_at_start_of_current_trial = 0\n",
    "\n",
    "    # You should reach convergence well before this\n",
    "    max_failures = 500\n",
    "\n",
    "    # Initialize a cart pole\n",
    "    cart_pole = CartPole(Physics())\n",
    "\n",
    "    # Starting `state_tuple` is (0, 0, 0, 0)\n",
    "    # x, x_dot, theta, theta_dot represents the actual continuous state vector\n",
    "    x, x_dot, theta, theta_dot = 0.0, 0.0, 0.0, 0.0\n",
    "    state_tuple = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "    # `state` is the number given to this state, you only need to consider\n",
    "    # this representation of the state\n",
    "    state = cart_pole.get_state(state_tuple)\n",
    "    # if min_trial_length_to_start_display == 0 or display_started == 1:\n",
    "    #     cart_pole.show_cart(state_tuple, pause_time)\n",
    "\n",
    "    mdp_data = initialize_mdp_data(NUM_STATES)\n",
    "\n",
    "    # This is the criterion to end the simulation.\n",
    "    # You should change it to terminate when the previous\n",
    "    # 'NO_LEARNING_THRESHOLD' consecutive value function computations all\n",
    "    # converged within one value function iteration. Intuitively, it seems\n",
    "    # like there will be little learning after this, so end the simulation\n",
    "    # here, and say the overall algorithm has converged.\n",
    "\n",
    "    consecutive_no_learning_trials = 0\n",
    "    while consecutive_no_learning_trials < NO_LEARNING_THRESHOLD:\n",
    "\n",
    "        action = choose_action(state, mdp_data)\n",
    "\n",
    "        # Get the next state by simulating the dynamics\n",
    "        state_tuple = cart_pole.simulate(action, state_tuple)\n",
    "        # x, x_dot, theta, theta_dot = state_tuple\n",
    "\n",
    "        # Increment simulation time\n",
    "        time = time + 1\n",
    "\n",
    "        # Get the state number corresponding to new state vector\n",
    "        new_state = cart_pole.get_state(state_tuple)\n",
    "        # if display_started == 1:\n",
    "        #     cart_pole.show_cart(state_tuple, pause_time)\n",
    "\n",
    "        # reward function to use - do not change this!\n",
    "        if new_state == NUM_STATES - 1:\n",
    "            R = -1\n",
    "        else:\n",
    "            R = 0\n",
    "\n",
    "        update_mdp_transition_counts_reward_counts(mdp_data, state, action, new_state, R)\n",
    "\n",
    "        # Recompute MDP model whenever pole falls\n",
    "        # Compute the value function V for the new model\n",
    "        if new_state == NUM_STATES - 1:\n",
    "\n",
    "            update_mdp_transition_probs_reward(mdp_data)\n",
    "\n",
    "            converged_in_one_iteration = update_mdp_value(mdp_data, TOLERANCE, GAMMA)\n",
    "\n",
    "            if converged_in_one_iteration:\n",
    "                consecutive_no_learning_trials = consecutive_no_learning_trials + 1\n",
    "            else:\n",
    "                consecutive_no_learning_trials = 0\n",
    "\n",
    "        # Do NOT change this code: Controls the simulation, and handles the case\n",
    "        # when the pole fell and the state must be reinitialized.\n",
    "        if new_state == NUM_STATES - 1:\n",
    "            num_failures += 1\n",
    "            if num_failures >= max_failures:\n",
    "                break\n",
    "            print('[INFO] Failure number {}'.format(num_failures))\n",
    "            time_steps_to_failure.append(time - time_at_start_of_current_trial)\n",
    "            # time_steps_to_failure[num_failures] = time - time_at_start_of_current_trial\n",
    "            time_at_start_of_current_trial = time\n",
    "\n",
    "            if time_steps_to_failure[num_failures - 1] > min_trial_length_to_start_display:\n",
    "                display_started = 1\n",
    "\n",
    "            # Reinitialize state\n",
    "            # x = 0.0\n",
    "            x = -1.1 + np.random.uniform() * 2.2\n",
    "            x_dot, theta, theta_dot = 0.0, 0.0, 0.0\n",
    "            state_tuple = (x, x_dot, theta, theta_dot)\n",
    "            state = cart_pole.get_state(state_tuple)\n",
    "        else:\n",
    "            state = new_state\n",
    "\n",
    "    if plot:\n",
    "        # plot the learning curve (time balanced vs. trial)\n",
    "        log_tstf = np.log(np.array(time_steps_to_failure))\n",
    "        plt.plot(np.arange(len(time_steps_to_failure)), log_tstf, 'k')\n",
    "        window = 30\n",
    "        w = np.array([1/window for _ in range(window)])\n",
    "        weights = lfilter(w, 1, log_tstf)\n",
    "        x = np.arange(window//2, len(log_tstf) - window//2)\n",
    "        plt.plot(x, weights[window:len(log_tstf)], 'r--')\n",
    "        plt.xlabel('Num failures')\n",
    "        plt.ylabel('Log of num steps to failure')\n",
    "        plt.title('seed = {}'.format(seed))\n",
    "        plt.savefig('/home/anhnt02/Desktop/CS229-Fall2018-FullCourse/Full_Problem_Set_Solution/PS4/output/control_{}.png'.format(seed))\n",
    "\n",
    "    return np.array(time_steps_to_failure)"
   ],
   "id": "5fada261f99dd59a",
   "outputs": [],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T16:47:05.249514Z",
     "start_time": "2025-08-31T16:47:02.956056Z"
    }
   },
   "cell_type": "code",
   "source": "main()",
   "id": "bc16cb4f7dc4cc08",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Failure number 1\n",
      "[INFO] Failure number 2\n",
      "[INFO] Failure number 3\n",
      "[INFO] Failure number 4\n",
      "[INFO] Failure number 5\n",
      "[INFO] Failure number 6\n",
      "[INFO] Failure number 7\n",
      "[INFO] Failure number 8\n",
      "[INFO] Failure number 9\n",
      "[INFO] Failure number 10\n",
      "[INFO] Failure number 11\n",
      "[INFO] Failure number 12\n",
      "[INFO] Failure number 13\n",
      "[INFO] Failure number 14\n",
      "[INFO] Failure number 15\n",
      "[INFO] Failure number 16\n",
      "[INFO] Failure number 17\n",
      "[INFO] Failure number 18\n",
      "[INFO] Failure number 19\n",
      "[INFO] Failure number 20\n",
      "[INFO] Failure number 21\n",
      "[INFO] Failure number 22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 15,  17,  17,  22,  23,  22,  22,  22,  20,  22,  22,  21,  21,\n",
       "        20,  23, 164, 135, 147,  19, 109, 108, 108])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "execution_count": 118
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T16:47:05.318149Z",
     "start_time": "2025-08-31T16:47:05.315395Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9cebdae802d55dab",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
