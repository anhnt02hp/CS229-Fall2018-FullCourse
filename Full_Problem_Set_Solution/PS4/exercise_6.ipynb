{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "# PS4-6: Reinforcement Learning: The inverted pendulum",
   "id": "e957a34f540bccb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Library",
   "id": "65bb05c2f801c4a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T04:08:46.472481Z",
     "start_time": "2025-09-01T04:08:46.467077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import division, print_function\n",
    "from src.env import CartPole, Physics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import lfilter"
   ],
   "id": "41c9d2cd2fefb3a0",
   "outputs": [],
   "execution_count": 169
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training RL Model",
   "id": "5d736ead2ccda755"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training Algorithm",
   "id": "52c1b4ee6c8c9cca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T04:08:46.494460Z",
     "start_time": "2025-09-01T04:08:46.486261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_mdp_data(num_states):\n",
    "    \"\"\"\n",
    "    Return a variable that contains all the parameters/state you need for your MDP.\n",
    "    Feel free to use whatever data type is most convenient for you (custom classes, tuples, dicts, etc)\n",
    "\n",
    "    Assume that no transitions or rewards have been observed.\n",
    "    Initialize the value function array to small random values (0 to 0.10, say).\n",
    "    Initialize the transition probabilities uniformly (ie, probability of\n",
    "        transitioning for state x to state y using action a is exactly\n",
    "        1/num_states).\n",
    "    Initialize all state rewards to zero.\n",
    "\n",
    "    Args:\n",
    "        num_states: The number of states\n",
    "\n",
    "    Returns: The initial MDP parameters\n",
    "    \"\"\"\n",
    "    transition_counts = np.zeros((num_states, num_states, 2))               # 3D array: (s, s', a)\n",
    "    transition_probs = np.ones((num_states, num_states, 2)) / num_states    # 3D array: (s, s', a)\n",
    "    # Index zero is count of rewards being -1 , index 1 is count of total num state is reached\n",
    "    reward_counts = np.zeros((num_states, 2))           # 2D array: (s, a)\n",
    "    reward = np.zeros(num_states)                       # 1D array: (s,)\n",
    "    value = np.random.rand(num_states) * 0.1            # 1D array: (s,)\n",
    "\n",
    "    return {\n",
    "        'transition_counts': transition_counts,\n",
    "        'transition_probs': transition_probs,\n",
    "        'reward_counts': reward_counts,\n",
    "        'reward': reward,\n",
    "        'value': value,\n",
    "        'num_states': num_states,\n",
    "    }"
   ],
   "id": "f5a86f4f339d7f69",
   "outputs": [],
   "execution_count": 170
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T04:08:46.554537Z",
     "start_time": "2025-09-01T04:08:46.543186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def choose_action(state, mdp_data):\n",
    "    \"\"\"\n",
    "    Choose the next action (0 or 1) that is optimal according to your current\n",
    "    mdp_data. When there is no optimal action, return a random action.\n",
    "\n",
    "    Args:\n",
    "        state: The current state in the MDP\n",
    "        mdp_data: The parameters for your MDP. See initialize_mdp_data.\n",
    "\n",
    "    Returns:\n",
    "        0 or 1 that is optimal according to your current MDP\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "\n",
    "    # 1. Initialize Current Parameters at State s\n",
    "    transition_probs = mdp_data['transition_probs']\n",
    "    value = mdp_data['value']\n",
    "    num_states = mdp_data['num_states']\n",
    "    num_actions = 2         # Because we only have 2 action: 0 or 1\n",
    "\n",
    "    # 2. Calculate V(s) for each action\n",
    "    #V(s) = ∑(s'∈S) Psa(s')*V(s')\n",
    "    action_value = np.zeros(num_actions)\n",
    "    for a in range(num_actions):\n",
    "        for s_next in range(num_states):\n",
    "            action_value[a] += transition_probs[state, s_next, a] * value[s_next]\n",
    "\n",
    "    # 3. Get Optimal Value Action V*(s) & Choose Optimal Action a*\n",
    "    #V*(s) = max_(a∈A)(V(s)) => a* = argMax(a∈A)(V(s))\n",
    "    optimal_value = -1e9\n",
    "    optimal_action = 0\n",
    "    cnt = 0\n",
    "\n",
    "    for a in range(num_actions):\n",
    "        if optimal_value < action_value[a]:\n",
    "            optimal_value = action_value[a]\n",
    "            optimal_action = a\n",
    "            cnt = 1\n",
    "        elif optimal_value == action_value[a]:\n",
    "            cnt += 1\n",
    "\n",
    "    # 4. If no optimal action, return a random action\n",
    "    if cnt > 1:\n",
    "        actions = [0, 1]\n",
    "        optimal_action = np.random.choice(actions)\n",
    "\n",
    "    return optimal_action\n",
    "    # *** END CODE HERE ***"
   ],
   "id": "299a34a38aef4de9",
   "outputs": [],
   "execution_count": 171
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T04:08:46.611630Z",
     "start_time": "2025-09-01T04:08:46.603119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_mdp_transition_counts_reward_counts(mdp_data, state, action, new_state, reward):\n",
    "    \"\"\"\n",
    "    Update the transition count and reward count information in your mdp_data.\n",
    "    Do not change the other MDP parameters (those get changed later).\n",
    "\n",
    "    Record the number of times `state, action, new_state` occurs.\n",
    "    Record the rewards for every `new_state`\n",
    "    (since rewards are -1 or 0, you just need to record number of times reward -1 is seen in 'reward_counts' index new_state,0)\n",
    "    Record the number of time `new_state` was reached (in 'reward_counts' index new_state,1)\n",
    "\n",
    "    Args:\n",
    "        mdp_data: The parameters of your MDP. See initialize_mdp_data.\n",
    "        state: The state that was observed at the start.\n",
    "        action: The action you performed.\n",
    "        new_state: The state after your action.\n",
    "        reward: The reward after your action (i.e. reward corresponding to new_state).\n",
    "\n",
    "    Returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    # 1. Get All Current Parameters at State s\n",
    "    transition_counts = mdp_data['transition_counts']\n",
    "    reward_counts = mdp_data['reward_counts']\n",
    "\n",
    "    # 2. Update transition_counts\n",
    "    transition_counts[state, new_state, action] += 1\n",
    "\n",
    "    # 3. Update reward_counts\n",
    "    if reward == -1:\n",
    "        reward_counts[new_state, 0] += 1\n",
    "\n",
    "    reward_counts[new_state, 1] += 1\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "    # This function does not return anything\n",
    "    return"
   ],
   "id": "3fce76ce348bcdcb",
   "outputs": [],
   "execution_count": 172
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T04:08:46.674479Z",
     "start_time": "2025-09-01T04:08:46.660749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_mdp_transition_probs_reward(mdp_data):\n",
    "    \"\"\"\n",
    "    Update the estimated transition probabilities and reward values in your MDP.\n",
    "\n",
    "    Make sure you account for the case when a state-action pair has never\n",
    "    been tried before, or the state has never been visited before. In that\n",
    "    case, you must not change that component (and thus keep it at the\n",
    "    initialized uniform distribution).\n",
    "\n",
    "    Args:\n",
    "        mdp_data: The data for your MDP. See initialize_mdp_data.\n",
    "\n",
    "    Returns:\n",
    "        Nothing\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    # 1. Get All Current Parameters at State s\n",
    "    transition_counts = mdp_data['transition_counts']\n",
    "    transition_probs = mdp_data['transition_probs']\n",
    "    reward_counts = mdp_data['reward_counts']\n",
    "    reward = mdp_data['reward']\n",
    "    num_states = mdp_data['num_states']\n",
    "    num_actions = 2         # Because we only have 2 action: 0 or 1\n",
    "\n",
    "    # 2. Update reward R(s)\n",
    "    for s in range(num_states):\n",
    "        if reward_counts[s, 1] > 0:\n",
    "            reward[s] = -1 * reward_counts[s, 0] / reward_counts[s, 1]\n",
    "\n",
    "    # 3. Update transition_probs Psa(s')\n",
    "    for s in range(num_states):\n",
    "        for a in range(num_actions):\n",
    "            total = 0\n",
    "            for s_next in range(num_states):\n",
    "                total += transition_counts[s, s_next, a]\n",
    "\n",
    "            if total > 0:\n",
    "                #update transition_probs\n",
    "                for s_next in range(num_states):\n",
    "                    transition_probs[s, s_next, a] = transition_counts[s, s_next, a] / total\n",
    "\n",
    "    # 4. Save back\n",
    "    mdp_data['reward'] = reward\n",
    "    mdp_data['transition_probs'] = transition_probs\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "    # This function does not return anything\n",
    "    return"
   ],
   "id": "7f67507e5e00a03f",
   "outputs": [],
   "execution_count": 173
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T04:08:46.738925Z",
     "start_time": "2025-09-01T04:08:46.725657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_mdp_value(mdp_data, tolerance, gamma):\n",
    "    \"\"\"\n",
    "    Update the estimated values in your MDP.\n",
    "\n",
    "    Perform value iteration using the new estimated model for the MDP.\n",
    "    The convergence criterion should be based on `TOLERANCE` as described\n",
    "    at the top of the file.\n",
    "\n",
    "    Return true if it converges within one iteration.\n",
    "\n",
    "    Args:\n",
    "        mdp_data: The data for your MDP. See initialize_mdp_data.\n",
    "        tolerance: The tolerance to use for the convergence criterion.\n",
    "        gamma: Your discount factor.\n",
    "\n",
    "    Returns:\n",
    "        True if the value iteration converged in one iteration\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "\n",
    "    # 1. Get All Current Parameters in State s\n",
    "    transition_probs = mdp_data['transition_probs']\n",
    "    reward = mdp_data['reward']\n",
    "    value = mdp_data['value']\n",
    "    num_states = mdp_data['num_states']\n",
    "    num_actions = 2\n",
    "\n",
    "    # 2. Update MDP\n",
    "    old_value = value.copy()\n",
    "\n",
    "    for s in range(num_states):\n",
    "        action_values = np.zeros(num_actions)\n",
    "        for a in range(num_actions):\n",
    "            for s_next in range(num_states):\n",
    "                action_values[a] += transition_probs[s, s_next, a] * old_value[s_next]\n",
    "            action_values[a] = reward[s] + gamma * action_values[a]\n",
    "\n",
    "        # 3. Find V*\n",
    "        optimal_value = -1e9\n",
    "        for a in range(num_actions):\n",
    "            if optimal_value < action_values[a]:\n",
    "                optimal_value = action_values[a]\n",
    "        value[s] = optimal_value\n",
    "\n",
    "    # 4. Check convergence\n",
    "    converged = False\n",
    "    cnt = 0\n",
    "    for s in range(num_states):\n",
    "        if abs(value[s] - old_value[s]) < tolerance:\n",
    "            cnt += 1\n",
    "    if cnt == num_states:\n",
    "        converged = True\n",
    "\n",
    "    # 5. Save back\n",
    "    mdp_data['value'] = value\n",
    "    return converged\n",
    "    # *** END CODE HERE ***"
   ],
   "id": "30ee6a54069e176f",
   "outputs": [],
   "execution_count": 174
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training Model",
   "id": "69eb52c98149cb17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T04:08:46.828371Z",
     "start_time": "2025-09-01T04:08:46.790409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main(plot=True):\n",
    "    # Seed the randomness of the simulation so this outputs the same thing each time\n",
    "    seed = 0\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Simulation parameters\n",
    "    pause_time = 0.0001\n",
    "    min_trial_length_to_start_display = 100\n",
    "    display_started = min_trial_length_to_start_display == 0\n",
    "\n",
    "    NUM_STATES = 163\n",
    "    GAMMA = 0.995\n",
    "    TOLERANCE = 0.01\n",
    "    NO_LEARNING_THRESHOLD = 20\n",
    "\n",
    "    # Time cycle of the simulation\n",
    "    time = 0\n",
    "\n",
    "    # These variables perform bookkeeping (how many cycles was the pole\n",
    "    # balanced for before it fell). Useful for plotting learning curves.\n",
    "    time_steps_to_failure = []\n",
    "    num_failures = 0\n",
    "    time_at_start_of_current_trial = 0\n",
    "\n",
    "    # You should reach convergence well before this\n",
    "    max_failures = 500\n",
    "\n",
    "    # Initialize a cart pole\n",
    "    cart_pole = CartPole(Physics())\n",
    "\n",
    "    # Starting `state_tuple` is (0, 0, 0, 0)\n",
    "    # x, x_dot, theta, theta_dot represents the actual continuous state vector\n",
    "    x, x_dot, theta, theta_dot = 0.0, 0.0, 0.0, 0.0\n",
    "    state_tuple = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "    # `state` is the number given to this state, you only need to consider\n",
    "    # this representation of the state\n",
    "    state = cart_pole.get_state(state_tuple)\n",
    "    # if min_trial_length_to_start_display == 0 or display_started == 1:\n",
    "    #     cart_pole.show_cart(state_tuple, pause_time)\n",
    "\n",
    "    mdp_data = initialize_mdp_data(NUM_STATES)\n",
    "\n",
    "    # This is the criterion to end the simulation.\n",
    "    # You should change it to terminate when the previous\n",
    "    # 'NO_LEARNING_THRESHOLD' consecutive value function computations all\n",
    "    # converged within one value function iteration. Intuitively, it seems\n",
    "    # like there will be little learning after this, so end the simulation\n",
    "    # here, and say the overall algorithm has converged.\n",
    "\n",
    "    consecutive_no_learning_trials = 0\n",
    "    while consecutive_no_learning_trials < NO_LEARNING_THRESHOLD:\n",
    "\n",
    "        action = choose_action(state, mdp_data)\n",
    "\n",
    "        # Get the next state by simulating the dynamics\n",
    "        state_tuple = cart_pole.simulate(action, state_tuple)\n",
    "        # x, x_dot, theta, theta_dot = state_tuple\n",
    "\n",
    "        # Increment simulation time\n",
    "        time = time + 1\n",
    "\n",
    "        # Get the state number corresponding to new state vector\n",
    "        new_state = cart_pole.get_state(state_tuple)\n",
    "        # if display_started == 1:\n",
    "        #     cart_pole.show_cart(state_tuple, pause_time)\n",
    "\n",
    "        # reward function to use - do not change this!\n",
    "        if new_state == NUM_STATES - 1:\n",
    "            R = -1\n",
    "        else:\n",
    "            R = 0\n",
    "\n",
    "        update_mdp_transition_counts_reward_counts(mdp_data, state, action, new_state, R)\n",
    "\n",
    "        # Recompute MDP model whenever pole falls\n",
    "        # Compute the value function V for the new model\n",
    "        if new_state == NUM_STATES - 1:\n",
    "\n",
    "            update_mdp_transition_probs_reward(mdp_data)\n",
    "\n",
    "            converged_in_one_iteration = update_mdp_value(mdp_data, TOLERANCE, GAMMA)\n",
    "\n",
    "            if converged_in_one_iteration:\n",
    "                consecutive_no_learning_trials = consecutive_no_learning_trials + 1\n",
    "            else:\n",
    "                consecutive_no_learning_trials = 0\n",
    "\n",
    "        # Do NOT change this code: Controls the simulation, and handles the case\n",
    "        # when the pole fell and the state must be reinitialized.\n",
    "        if new_state == NUM_STATES - 1:\n",
    "            num_failures += 1\n",
    "            if num_failures >= max_failures:\n",
    "                break\n",
    "            print('[INFO] Failure number {}'.format(num_failures))\n",
    "            time_steps_to_failure.append(time - time_at_start_of_current_trial)\n",
    "            # time_steps_to_failure[num_failures] = time - time_at_start_of_current_trial\n",
    "            time_at_start_of_current_trial = time\n",
    "\n",
    "            if time_steps_to_failure[num_failures - 1] > min_trial_length_to_start_display:\n",
    "                display_started = 1\n",
    "\n",
    "            # Reinitialize state\n",
    "            # x = 0.0\n",
    "            x = -1.1 + np.random.uniform() * 2.2\n",
    "            x_dot, theta, theta_dot = 0.0, 0.0, 0.0\n",
    "            state_tuple = (x, x_dot, theta, theta_dot)\n",
    "            state = cart_pole.get_state(state_tuple)\n",
    "        else:\n",
    "            state = new_state\n",
    "\n",
    "    if plot:\n",
    "        # plot the learning curve (time balanced vs. trial)\n",
    "        log_tstf = np.log(np.array(time_steps_to_failure))\n",
    "        plt.plot(np.arange(len(time_steps_to_failure)), log_tstf, 'k')\n",
    "        window = 30\n",
    "        w = np.array([1/window for _ in range(window)])\n",
    "        weights = lfilter(w, 1, log_tstf)\n",
    "        x = np.arange(window//2, len(log_tstf) - window//2)\n",
    "        plt.plot(x, weights[window:len(log_tstf)], 'r--')\n",
    "        plt.xlabel('Num failures')\n",
    "        plt.ylabel('Log of num steps to failure')\n",
    "        plt.title('seed = {}'.format(seed))\n",
    "        plt.savefig('/home/anhnt02/Desktop/CS229-Fall2018-FullCourse/Full_Problem_Set_Solution/PS4/output/control_{}.png'.format(seed))\n",
    "\n",
    "    return np.array(time_steps_to_failure)"
   ],
   "id": "5fada261f99dd59a",
   "outputs": [],
   "execution_count": 175
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T04:09:02.654724Z",
     "start_time": "2025-09-01T04:08:46.852016Z"
    }
   },
   "cell_type": "code",
   "source": "main()",
   "id": "bc16cb4f7dc4cc08",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Failure number 1\n",
      "[INFO] Failure number 2\n",
      "[INFO] Failure number 3\n",
      "[INFO] Failure number 4\n",
      "[INFO] Failure number 5\n",
      "[INFO] Failure number 6\n",
      "[INFO] Failure number 7\n",
      "[INFO] Failure number 8\n",
      "[INFO] Failure number 9\n",
      "[INFO] Failure number 10\n",
      "[INFO] Failure number 11\n",
      "[INFO] Failure number 12\n",
      "[INFO] Failure number 13\n",
      "[INFO] Failure number 14\n",
      "[INFO] Failure number 15\n",
      "[INFO] Failure number 16\n",
      "[INFO] Failure number 17\n",
      "[INFO] Failure number 18\n",
      "[INFO] Failure number 19\n",
      "[INFO] Failure number 20\n",
      "[INFO] Failure number 21\n",
      "[INFO] Failure number 22\n",
      "[INFO] Failure number 23\n",
      "[INFO] Failure number 24\n",
      "[INFO] Failure number 25\n",
      "[INFO] Failure number 26\n",
      "[INFO] Failure number 27\n",
      "[INFO] Failure number 28\n",
      "[INFO] Failure number 29\n",
      "[INFO] Failure number 30\n",
      "[INFO] Failure number 31\n",
      "[INFO] Failure number 32\n",
      "[INFO] Failure number 33\n",
      "[INFO] Failure number 34\n",
      "[INFO] Failure number 35\n",
      "[INFO] Failure number 36\n",
      "[INFO] Failure number 37\n",
      "[INFO] Failure number 38\n",
      "[INFO] Failure number 39\n",
      "[INFO] Failure number 40\n",
      "[INFO] Failure number 41\n",
      "[INFO] Failure number 42\n",
      "[INFO] Failure number 43\n",
      "[INFO] Failure number 44\n",
      "[INFO] Failure number 45\n",
      "[INFO] Failure number 46\n",
      "[INFO] Failure number 47\n",
      "[INFO] Failure number 48\n",
      "[INFO] Failure number 49\n",
      "[INFO] Failure number 50\n",
      "[INFO] Failure number 51\n",
      "[INFO] Failure number 52\n",
      "[INFO] Failure number 53\n",
      "[INFO] Failure number 54\n",
      "[INFO] Failure number 55\n",
      "[INFO] Failure number 56\n",
      "[INFO] Failure number 57\n",
      "[INFO] Failure number 58\n",
      "[INFO] Failure number 59\n",
      "[INFO] Failure number 60\n",
      "[INFO] Failure number 61\n",
      "[INFO] Failure number 62\n",
      "[INFO] Failure number 63\n",
      "[INFO] Failure number 64\n",
      "[INFO] Failure number 65\n",
      "[INFO] Failure number 66\n",
      "[INFO] Failure number 67\n",
      "[INFO] Failure number 68\n",
      "[INFO] Failure number 69\n",
      "[INFO] Failure number 70\n",
      "[INFO] Failure number 71\n",
      "[INFO] Failure number 72\n",
      "[INFO] Failure number 73\n",
      "[INFO] Failure number 74\n",
      "[INFO] Failure number 75\n",
      "[INFO] Failure number 76\n",
      "[INFO] Failure number 77\n",
      "[INFO] Failure number 78\n",
      "[INFO] Failure number 79\n",
      "[INFO] Failure number 80\n",
      "[INFO] Failure number 81\n",
      "[INFO] Failure number 82\n",
      "[INFO] Failure number 83\n",
      "[INFO] Failure number 84\n",
      "[INFO] Failure number 85\n",
      "[INFO] Failure number 86\n",
      "[INFO] Failure number 87\n",
      "[INFO] Failure number 88\n",
      "[INFO] Failure number 89\n",
      "[INFO] Failure number 90\n",
      "[INFO] Failure number 91\n",
      "[INFO] Failure number 92\n",
      "[INFO] Failure number 93\n",
      "[INFO] Failure number 94\n",
      "[INFO] Failure number 95\n",
      "[INFO] Failure number 96\n",
      "[INFO] Failure number 97\n",
      "[INFO] Failure number 98\n",
      "[INFO] Failure number 99\n",
      "[INFO] Failure number 100\n",
      "[INFO] Failure number 101\n",
      "[INFO] Failure number 102\n",
      "[INFO] Failure number 103\n",
      "[INFO] Failure number 104\n",
      "[INFO] Failure number 105\n",
      "[INFO] Failure number 106\n",
      "[INFO] Failure number 107\n",
      "[INFO] Failure number 108\n",
      "[INFO] Failure number 109\n",
      "[INFO] Failure number 110\n",
      "[INFO] Failure number 111\n",
      "[INFO] Failure number 112\n",
      "[INFO] Failure number 113\n",
      "[INFO] Failure number 114\n",
      "[INFO] Failure number 115\n",
      "[INFO] Failure number 116\n",
      "[INFO] Failure number 117\n",
      "[INFO] Failure number 118\n",
      "[INFO] Failure number 119\n",
      "[INFO] Failure number 120\n",
      "[INFO] Failure number 121\n",
      "[INFO] Failure number 122\n",
      "[INFO] Failure number 123\n",
      "[INFO] Failure number 124\n",
      "[INFO] Failure number 125\n",
      "[INFO] Failure number 126\n",
      "[INFO] Failure number 127\n",
      "[INFO] Failure number 128\n",
      "[INFO] Failure number 129\n",
      "[INFO] Failure number 130\n",
      "[INFO] Failure number 131\n",
      "[INFO] Failure number 132\n",
      "[INFO] Failure number 133\n",
      "[INFO] Failure number 134\n",
      "[INFO] Failure number 135\n",
      "[INFO] Failure number 136\n",
      "[INFO] Failure number 137\n",
      "[INFO] Failure number 138\n",
      "[INFO] Failure number 139\n",
      "[INFO] Failure number 140\n",
      "[INFO] Failure number 141\n",
      "[INFO] Failure number 142\n",
      "[INFO] Failure number 143\n",
      "[INFO] Failure number 144\n",
      "[INFO] Failure number 145\n",
      "[INFO] Failure number 146\n",
      "[INFO] Failure number 147\n",
      "[INFO] Failure number 148\n",
      "[INFO] Failure number 149\n",
      "[INFO] Failure number 150\n",
      "[INFO] Failure number 151\n",
      "[INFO] Failure number 152\n",
      "[INFO] Failure number 153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 15,  17,  17,  21,  20,  20,  14,  11,  13,  21,  45,  47,  47,\n",
       "        47,  47,  47,  47,  47,  47,  47,  47,  47,  47,  47,  17,  47,\n",
       "        47,  47,  47,  47,  47,  47,  47,  47,  47,  47,  47,  47,  47,\n",
       "        47,  47,  47,  47,  47,  47,  47,  47,  47,  47,  47,  47,  47,\n",
       "        47,  47,  47,  47,  47,  47,  47,  47,  47,  68,  60, 140, 112,\n",
       "       112, 112, 112,  96, 112, 111, 111, 111, 111, 111, 109, 111, 111,\n",
       "       111, 111, 111, 111, 108, 111, 111, 111, 108, 111, 111, 111, 111,\n",
       "       111, 111, 111, 111, 111, 129, 111, 111, 111, 111, 127, 111, 111,\n",
       "       111, 111, 111, 111, 111, 111, 111, 111, 138, 111, 135, 111, 111,\n",
       "       111, 111, 111, 137, 111, 111, 111, 111, 111, 111, 111, 111, 111,\n",
       "       111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111,\n",
       "       111, 111, 111, 111, 111, 111, 111, 111, 111, 111])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "execution_count": 176
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T04:09:02.744791Z",
     "start_time": "2025-09-01T04:09:02.741900Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9cebdae802d55dab",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
