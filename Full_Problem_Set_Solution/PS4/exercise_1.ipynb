{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "# PS4-1: Neural Networks: MNIST Image Classification",
   "id": "5ef2d0779a09a5b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Library",
   "id": "96393687726f3b9c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:08:46.953061Z",
     "start_time": "2025-09-03T15:08:46.947431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ],
   "id": "af4ac1e784f2cb92",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Global Parameters",
   "id": "30c2bd9c4dc33a34"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:08:47.007432Z",
     "start_time": "2025-09-03T15:08:47.003268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_POOL_SIZE = 5\n",
    "CONVOLUTION_SIZE = 4\n",
    "CONVOLUTION_FILTERS = 2"
   ],
   "id": "573b1d5abad9eb94",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training NN Algorithm",
   "id": "3e0bc68626bbc8b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model Architcture",
   "id": "f73616598932fb8b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:08:47.066775Z",
     "start_time": "2025-09-03T15:08:47.058266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def forward_softmax(x):\n",
    "    \"\"\"\n",
    "    Compute softmax function for a single example.\n",
    "    The shape of the input is of size # num classes.\n",
    "\n",
    "    Important Note: You must be careful to avoid overflow for this function. Functions\n",
    "    like softmax have a tendency to overflow when very large numbers like e^10000 are computed.\n",
    "    You will know that your function is overflow resistent when it can handle input like:\n",
    "    np.array([[10000, 10010, 10]]) without issues.\n",
    "\n",
    "        x: A 1d numpy float array of shape number_of_classes\n",
    "\n",
    "    Returns:\n",
    "        A 1d numpy float array containing the softmax results of shape  number_of_classes\n",
    "    \"\"\"\n",
    "    x = x - np.max(x,axis=0)\n",
    "    exp = np.exp(x)\n",
    "    s = exp / np.sum(exp,axis=0)\n",
    "    return s"
   ],
   "id": "ec594d5c6f7ddd9b",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:08:47.126735Z",
     "start_time": "2025-09-03T15:08:47.117522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def backward_softmax(x, grad_outputs):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with respect to x.\n",
    "\n",
    "    grad_outputs is the gradient of the loss with respect to the outputs of the softmax.\n",
    "\n",
    "    Args:\n",
    "        x: A 1d numpy float array of shape number_of_classes\n",
    "        grad_outputs: A 1d numpy float array of shape number_of_classes\n",
    "\n",
    "    Returns:\n",
    "        A 1d numpy float array of the same shape as x with the derivative of the loss with respect to x\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    \"\"\"\n",
    "    We need to compute derivative of CE loss w.r.t x\n",
    "    Note: x = logits (input of softmax) => x is 1D, y_hat = softmax(x)\n",
    "    Formula: dCE/dx = dCE/dy_hat * dy_hat/dx (Chain Rule), where:\n",
    "        dCE/dy_hat = grad_outputs\n",
    "        dy_hat/dx = Derivative of y_hat w.r.t x\n",
    "    \"\"\"\n",
    "    # 1. Get x size = number_of_classes\n",
    "    K = len(x)\n",
    "\n",
    "    # 2. Calculate Softmax\n",
    "    y_hat = forward_softmax(x)\n",
    "\n",
    "    # 3. Update dCE/dx\n",
    "    d_CE_x = np.zeros(K)\n",
    "\n",
    "    for j in range(K):\n",
    "        for k in range(K):\n",
    "            if j == k:\n",
    "                d_CE_x[j] += grad_outputs[k] * y_hat[k] * (1 - y_hat[k])\n",
    "            else:\n",
    "                d_CE_x[j] += grad_outputs[k] * (- y_hat[k] * y_hat[j])\n",
    "\n",
    "    return d_CE_x\n",
    "    # *** END CODE HERE ***"
   ],
   "id": "ce0ddf35c0a83b01",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:08:47.182368Z",
     "start_time": "2025-09-03T15:08:47.177384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def forward_relu(x):\n",
    "    \"\"\"\n",
    "    Compute the relu function for the input x.\n",
    "\n",
    "    Args:\n",
    "        x: A numpy float array\n",
    "\n",
    "    Returns:\n",
    "        A numpy float array containing the relu results\n",
    "    \"\"\"\n",
    "    x[x<=0] = 0\n",
    "\n",
    "    return x"
   ],
   "id": "fddfbb65a14a33f1",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:08:47.242721Z",
     "start_time": "2025-09-03T15:08:47.232105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def backward_relu(x, grad_outputs):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with respect to x\n",
    "\n",
    "    Args:\n",
    "        x: A numpy array of arbitrary shape containing the input.\n",
    "        grad_outputs: A numpy array of the same shape of x containing the gradient of the loss with respect\n",
    "            to the output of relu\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of the same shape as x containing the gradients with respect to x.\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    \"\"\"\n",
    "    We need to compute derivative of \"Cross-Entropy Loss is calculated after Softmax\" loss w.r.t x\n",
    "    Note: x = MaxPooling output (input of ReLU) => x is 3D image (Batch Size, Channels, Heights, Widths)\n",
    "    Formula: dL/dx = dL/reLU * dreLU/dx (Chain Rule), where:\n",
    "        dL/dreLU = grad_outputs\n",
    "        dreLU/dx  = Derivative of reLU w.r.t x\n",
    "    \"\"\"\n",
    "    # 1. Get x shape\n",
    "    C, H, W = x.shape\n",
    "\n",
    "    # 2. Update dL/dreLU\n",
    "    grad_x = np.zeros(x.shape)\n",
    "    for c in range(C):\n",
    "        for h in range(H):\n",
    "            for w in range(W):\n",
    "                if x[c, h, w] > 0:\n",
    "                    grad_x[c, h, w] = 1 * grad_outputs[c, h, w]\n",
    "                else:\n",
    "                    grad_x[c, h, w] = 0\n",
    "\n",
    "    return grad_x\n",
    "    # *** END CODE HERE ***"
   ],
   "id": "4da374176cb978bf",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:08:47.307213Z",
     "start_time": "2025-09-03T15:08:47.292666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_initial_params():\n",
    "    \"\"\"\n",
    "    Compute the initial parameters for the neural network.\n",
    "\n",
    "    This function should return a dictionary mapping parameter names to numpy arrays containing\n",
    "    the initial values for those parameters.\n",
    "\n",
    "    There should be four parameters for this model:\n",
    "    W1 is the weight matrix for the convolutional layer\n",
    "    b1 is the bias vector for the convolutional layer\n",
    "    W2 is the weight matrix for the output layers\n",
    "    b2 is the bias vector for the output layer\n",
    "\n",
    "    Weight matrices should be initialized with values drawn from a random normal distribution.\n",
    "    The mean of that distribution should be 0.\n",
    "    The variance of that distribution should be 1/sqrt(n) where n is the number of neurons that\n",
    "    feed into an output for that layer.\n",
    "\n",
    "    Bias vectors should be initialized with zero.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        A dict mapping parameter names to numpy arrays\n",
    "    \"\"\"\n",
    "\n",
    "    size_after_convolution = 28 - CONVOLUTION_SIZE + 1\n",
    "    size_after_max_pooling = size_after_convolution // MAX_POOL_SIZE\n",
    "\n",
    "    num_hidden = size_after_max_pooling * size_after_max_pooling * CONVOLUTION_FILTERS\n",
    "\n",
    "    return {\n",
    "        'W1': np.random.normal(size = (CONVOLUTION_FILTERS, 1, CONVOLUTION_SIZE, CONVOLUTION_SIZE), scale=1/ math.sqrt(CONVOLUTION_SIZE * CONVOLUTION_SIZE)),\n",
    "        'b1': np.zeros(CONVOLUTION_FILTERS),\n",
    "        'W2': np.random.normal(size = (num_hidden, 10), scale = 1/ math.sqrt(num_hidden)),\n",
    "        'b2': np.zeros(10)\n",
    "    }"
   ],
   "id": "a2eb67b0070ee17d",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:08:47.362471Z",
     "start_time": "2025-09-03T15:08:47.351209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def forward_convolution(conv_W, conv_b, data):\n",
    "    \"\"\"\n",
    "    Compute the output from a convolutional layer given the weights and data.\n",
    "\n",
    "    conv_W is of the shape (# output channels, # input channels, convolution width, convolution height )\n",
    "    conv_b is of the shape (# output channels)\n",
    "\n",
    "    data is of the shape (# input channels, width, height)\n",
    "\n",
    "    The output should be the result of a convolution and should be of the size:\n",
    "        (# output channels, width - convolution width + 1, height -  convolution height + 1)\n",
    "\n",
    "    Returns:\n",
    "        The output of the convolution as a numpy array\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    oc = output_channels\n",
    "    ic = input_channels\n",
    "    x = starting coordinates in width.\n",
    "    y = starting coordinates in height.\n",
    "    di = conv_W_offset_width\n",
    "    dj = conv_W_offset_height\n",
    "    data = input data\n",
    "\n",
    "    Formula:\n",
    "    output[oc, x, y] = conv_b[oc] + sum_di,sum_dj,sum_ic (data[ic, x + di, y + dj] * conv_W[oc, ic, di, dj])\n",
    "    \"\"\"\n",
    "    conv_channels, _, conv_width, conv_height = conv_W.shape\n",
    "\n",
    "    input_channels, input_width, input_height = data.shape\n",
    "\n",
    "    output = np.zeros((conv_channels, input_width - conv_width + 1, input_height - conv_height + 1))\n",
    "\n",
    "    for x in range(input_width - conv_width + 1):\n",
    "        for y in range(input_height - conv_height + 1):\n",
    "            for output_channel in range(conv_channels):\n",
    "                output[output_channel, x, y] = np.sum(\n",
    "                    np.multiply(data[:, x:(x + conv_width), y:(y + conv_height)], conv_W[output_channel, :, :, :])) + conv_b[output_channel]\n",
    "\n",
    "    return output"
   ],
   "id": "35700cc30c212df1",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:08:47.431668Z",
     "start_time": "2025-09-03T15:08:47.411923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def backward_convolution(conv_W, conv_b, data, output_grad):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with respect to the parameters of the convolution.\n",
    "\n",
    "    See forward_convolution for the sizes of the arguments.\n",
    "    output_grad is the gradient of the loss with respect to the output of the convolution.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing 3 gradients.\n",
    "        The first element is the gradient of the loss with respect to the convolution weights\n",
    "        The second element is the gradient of the loss with respect to the convolution bias\n",
    "        The third element is the gradient of the loss with respect to the input data\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    \"\"\"\n",
    "    We need to compute derivative of \"Loss Function\" loss w.r.t conv_W, conv_b, data in tuple\n",
    "\n",
    "    Parameters:\n",
    "    oc = output_channels\n",
    "    ic = input_channels\n",
    "    x = starting coordinates in width (data)\n",
    "    y = starting coordinates in height (data)\n",
    "    di = conv_W_offset_width\n",
    "    dj = conv_W_offset_height\n",
    "    data = input data\n",
    "\n",
    "    Formula:\n",
    "    dL/doutput = dL/doutput[oc, x, y] === output_grad\n",
    "\n",
    "    doutput/dconv_W = doutput[oc, x, y]/dconv_W[oc, ic, di, dj] === data[ic, x + di, y + dj]\n",
    "\n",
    "    doutput/dconv_b = doutput[oc, x, y]/dconv_b[oc] === 1\n",
    "\n",
    "    doutput/ddata = doutput[oc, x, y]/ddata[ic, x+di, y+dj] === conv_W[oc, ic, di, dj]\n",
    "\n",
    "    So, we have full Formula:\n",
    "    1. dL/dconv_W = dL/doutput * doutput/dconv_W === sum_x,sum_y (output_grad[oc, x, y] * data[ic, x + di, y + dj])\n",
    "\n",
    "    2. dL/dconv_b = dL/doutput * doutput/dconv_b === sum_x, sum_y (output_grad[oc, x, y] * 1)\n",
    "\n",
    "    3. dL/ddata = dL/doutput * doutput/ddata === sum_oc, sum_di, sum_dj (output_grad[oc, x-di, y-dj] * conv_W[oc, ic, di, dj])\n",
    "    \"\"\"\n",
    "    # 1. Get Parameters\n",
    "    conv_channels, _, conv_width, conv_height = conv_W.shape\n",
    "    input_channels, input_width, input_height = data.shape\n",
    "    _, output_width, output_height = output_grad.shape\n",
    "\n",
    "    # 2. Initialize Gradient\n",
    "    dW = np.zeros(conv_W.shape)\n",
    "    db = np.zeros(conv_b.shape)\n",
    "    ddata = np.zeros(data.shape)\n",
    "\n",
    "    # 3. dL/dconv_W\n",
    "    for oc in range(conv_channels):\n",
    "        for ic in range(input_channels):\n",
    "            for di in range(conv_width):\n",
    "                for dj in range(conv_height):\n",
    "                    temp_grad = 0\n",
    "                    for x in range(output_width):\n",
    "                        for y in range(output_height):\n",
    "                            temp_grad += output_grad[oc, x, y] * data[ic, x + di, y + dj]\n",
    "                    dW[oc, ic, di, dj] = temp_grad\n",
    "\n",
    "    # 4. dL/dconv_b\n",
    "    for oc in range(conv_channels):\n",
    "        temp_grad = 0\n",
    "        for x in range(output_width):\n",
    "            for y in range(output_height):\n",
    "                temp_grad += output_grad[oc, x, y] * 1\n",
    "        db[oc] = temp_grad\n",
    "\n",
    "    # 5. dL/ddata\n",
    "    for oc in range(conv_channels):\n",
    "        for ic in range(input_channels):\n",
    "            for x in range(input_width):\n",
    "                for y in range(input_height):\n",
    "                    temp_grad = 0\n",
    "                    for di in range(conv_width):\n",
    "                        for dj in range(conv_height):\n",
    "                            x_out = x - di\n",
    "                            y_out = y - dj\n",
    "                            if 0 <= x_out < output_width and 0 <= y_out < output_height:\n",
    "                                temp_grad += output_grad[oc, x_out, y_out] * conv_W[oc, ic, di, dj]\n",
    "                    ddata[ic, x, y] = temp_grad\n",
    "\n",
    "    return dW, db, ddata\n",
    "    # *** END CODE HERE ***"
   ],
   "id": "c15417e8b8c81c93",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:08:47.491843Z",
     "start_time": "2025-09-03T15:08:47.482722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def forward_max_pool(data, pool_width, pool_height):\n",
    "    \"\"\"\n",
    "    Compute the output from a max pooling layer given the data and pool dimensions.\n",
    "\n",
    "    The stride length should be equal to the pool size\n",
    "\n",
    "    data is of the shape (# channels, width, height)\n",
    "\n",
    "    The output should be the result of the max pooling layer and should be of size:\n",
    "        (# channels, width // pool_width, height // pool_height)\n",
    "\n",
    "    Returns:\n",
    "        The result of the max pooling layer\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    oc = output_channels\n",
    "    ic = input_channels\n",
    "    x = starting coordinates in width (data)\n",
    "    y = starting coordinates in height (data)\n",
    "    di = conv_W_offset_width\n",
    "    dj = conv_W_offset_height\n",
    "    data = input data\n",
    "\n",
    "    Formula:\n",
    "    output[oc, x, y] = max_di,dj (data[ic, x * pool_width + di, y * pool_height + dj])\n",
    "    \"\"\"\n",
    "    input_channels, input_width, input_height = data.shape\n",
    "\n",
    "    output = np.zeros((input_channels, input_width // pool_width, input_height // pool_height))\n",
    "\n",
    "    for x in range(0, input_width, pool_width):\n",
    "        for y in range(0, input_height, pool_height):\n",
    "\n",
    "            output[:, x // pool_width, y // pool_height] = np.amax(data[:, x:(x + pool_width), y:(y + pool_height)], axis=(1, 2))\n",
    "\n",
    "    return output"
   ],
   "id": "d913c8ec5222e122",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:08:47.556853Z",
     "start_time": "2025-09-03T15:08:47.540390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def backward_max_pool(data, pool_width, pool_height, output_grad):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with respect to the data in the max pooling layer.\n",
    "\n",
    "    data is of the shape (# channels, width, height)\n",
    "    output_grad is of shape (# channels, width // pool_width, height // pool_height)\n",
    "\n",
    "    output_grad is the gradient of the loss with respect to the output of the backward max\n",
    "    pool layer.\n",
    "\n",
    "    Returns:\n",
    "        The gradient of the loss with respect to the data (of same shape as data)\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    \"\"\"\n",
    "    We need to compute derivative of \"Loss Function\" loss w.r.t data\n",
    "\n",
    "    Parameters:\n",
    "    oc = output_channels\n",
    "    ic = input_channels\n",
    "    x = starting coordinates in width (data)\n",
    "    y = starting coordinates in height (data)\n",
    "    di = step of pool_width\n",
    "    dj = step of pool_height\n",
    "    data = input data\n",
    "    x_out = x/pool_width\n",
    "    y_out = y/pool_height\n",
    "\n",
    "    Formula:\n",
    "    dL/doutput = dL/doutput[oc, x, y] === output_grad[oc, x_out, y_out]\n",
    "\n",
    "    doutput/ddata = doutput[oc, x, y]/ddata[ic, x * pool_width + di, y * pool_height + dj] === 1 (if (i,j) is max in (x_out, y_out)) else === 0\n",
    "\n",
    "    So, we have ful Formula:\n",
    "    dL/ddata = dL/doutput * doutput/ddata === output_grad[oc, x_out, y_out] * 1 (if (i,j) is max in (x_out, y_out)) or === 0\n",
    "    \"\"\"\n",
    "    # 1. Get Parameters\n",
    "    input_channels, input_width, input_height = data.shape\n",
    "    X_out, Y_out = input_width // pool_width, input_height // pool_height\n",
    "    _, output_width, output_height = output_grad.shape\n",
    "\n",
    "    # 2. Initialize Parameters\n",
    "    ddata = np.zeros(data.shape)\n",
    "\n",
    "    # 3. dL/ddata\n",
    "    for ic in range(input_channels):\n",
    "        for x_out in range(X_out):\n",
    "            for y_out in range(Y_out):\n",
    "                max_i, max_j = 0, 0\n",
    "                max_value = -1e9\n",
    "                for di in range(pool_width):\n",
    "                    for dj in range(pool_height):\n",
    "                        val = data[ic, x_out * pool_width + di, y_out * pool_height + dj]\n",
    "                        if val > max_value:\n",
    "                            max_value = 1 * val\n",
    "                            max_i, max_j = di, dj\n",
    "                ddata[ic, x_out * pool_width + max_i, y_out * pool_height + max_j] = output_grad[ic, x_out, y_out]          #oc = ic\n",
    "\n",
    "    return ddata\n",
    "    # *** END CODE HERE ***"
   ],
   "id": "728e3e9a04a66846",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:08:47.608612Z",
     "start_time": "2025-09-03T15:08:47.602117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def forward_cross_entropy_loss(probabilities, labels):\n",
    "    \"\"\"\n",
    "    Compute the output from a cross entropy loss layer given the probabilities and labels.\n",
    "\n",
    "    probabilities is of the shape (# classes)\n",
    "    labels is of the shape (# classes)\n",
    "\n",
    "    The output should be a scalar\n",
    "\n",
    "    Returns:\n",
    "        The result of the log loss layer\n",
    "    \"\"\"\n",
    "\n",
    "    result = 0\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        if label == 1:\n",
    "            result += -np.log(probabilities[i])\n",
    "\n",
    "    return result"
   ],
   "id": "c242d0b702c5a9af",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:08:47.672524Z",
     "start_time": "2025-09-03T15:08:47.663121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def backward_cross_entropy_loss(probabilities, labels):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the cross entropy loss with respect to the probabilities.\n",
    "\n",
    "    probabilities is of the shape (# classes)\n",
    "    labels is of the shape (# classes)\n",
    "\n",
    "    The output should be the gradient with respect to the probabilities.\n",
    "\n",
    "    Returns:\n",
    "        The gradient of the loss with respect to the probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    \"\"\"\n",
    "    We need to estimate Derivative of CE Loss w.r.t y_hat_k (probabilities)\n",
    "\n",
    "    Formula:\n",
    "    dCE/dy_hat_k = -y_k/y_hat_k\n",
    "    With \"one-hot-labels\"   => dCE/dy_hat_k = -1/y_hat_k = -1/probabilities[k]\n",
    "                            => dCE/dy_hat_k = 0\n",
    "    \"\"\"\n",
    "    # 1. Get labels size\n",
    "    K = len(labels)\n",
    "\n",
    "    # 2. Initialize gradient\n",
    "    grad = np.zeros(K)\n",
    "\n",
    "    # 3. dCE/dx_k\n",
    "    for k in range(K):\n",
    "        if labels[k] == 1:\n",
    "            grad[k] = -1.0 / probabilities[k]\n",
    "        else:\n",
    "            grad[k] = 0.0\n",
    "\n",
    "    return grad\n",
    "    # *** END CODE HERE ***"
   ],
   "id": "eab5eacfa1c5024e",
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:08:47.731633Z",
     "start_time": "2025-09-03T15:08:47.724660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def forward_linear(weights, bias, data):\n",
    "    \"\"\"\n",
    "    Compute the output from a linear layer with the given weights, bias and data.\n",
    "    weights is of the shape (input # features, output # features)\n",
    "    bias is of the shape (output # features)\n",
    "    data is of the shape (input # features)\n",
    "\n",
    "    The output should be of the shape (output # features)\n",
    "\n",
    "    Returns:\n",
    "        The result of the linear layer\n",
    "    \"\"\"\n",
    "    return data.dot(weights) + bias"
   ],
   "id": "9f94740e7ed9370b",
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:08:47.794645Z",
     "start_time": "2025-09-03T15:08:47.782466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def backward_linear(weights, bias, data, output_grad):\n",
    "    \"\"\"\n",
    "    Compute the gradients of the loss with respect to the parameters of a linear layer.\n",
    "\n",
    "    See forward_linear for information about the shapes of the variables.\n",
    "\n",
    "    output_grad is the gradient of the loss with respect to the output of this layer.\n",
    "\n",
    "    This should return a tuple with three elements:\n",
    "    - The gradient of the loss with respect to the weights\n",
    "    - The gradient of the loss with respect to the bias\n",
    "    - The gradient of the loss with respect to the data\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    \"\"\"\n",
    "    We need to estimate \"Loss Function\" w.r.t weights, bias, data\n",
    "\n",
    "    Parameters:\n",
    "    weights = 2D (input_features, output_features)\n",
    "    bias = 1D (output_features)\n",
    "    data = 1D (input_features)\n",
    "\n",
    "    Formula:\n",
    "    Linear = x.W + b\n",
    "\n",
    "    dL/dLinear = output_grad\n",
    "    dL/dweights = dL/dLinear * dLinear/dweights === output_grad * data\n",
    "    dL/dbias = dL/dLinear * dLinear/dbias === output_grad * 1\n",
    "    dL/ddata = dL/dLinear * dLinear/ddata === output_grad * weights\n",
    "    \"\"\"\n",
    "    # 1. Get weights shape\n",
    "    input_features, output_features = weights.shape\n",
    "\n",
    "    # 2. Initialize gradients\n",
    "    dweights = np.zeros(weights.shape)\n",
    "    dbias = np.zeros(bias.shape)\n",
    "    ddata = np.zeros(data.shape)\n",
    "\n",
    "    # 3. dweights[i,j] = output_grad[j] * data[i]\n",
    "    for i in range(input_features):\n",
    "        for j in range(output_features):\n",
    "            dweights[i, j] = output_grad[j] * data[i]\n",
    "\n",
    "    # 4. dbias[j] = output_grad[j] * 1\n",
    "    for j in range(output_features):\n",
    "        dbias[j] = output_grad[j]\n",
    "\n",
    "    # 5. ddata[i] = output_grad[j] * weights[i, j]\n",
    "    for i in range(input_features):\n",
    "        temp = 0\n",
    "        for j in range(output_features):\n",
    "            temp += output_grad[j] * weights[i, j]\n",
    "        ddata[i] = temp\n",
    "\n",
    "    return dweights, dbias, ddata\n",
    "    # *** END CODE HERE ***"
   ],
   "id": "e178a42eb20f4370",
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:08:47.854809Z",
     "start_time": "2025-09-03T15:08:47.845234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def forward_prop(data, labels, params):\n",
    "    \"\"\"\n",
    "    Implement the forward layer given the data, labels, and params.\n",
    "\n",
    "    Args:\n",
    "        data: A numpy array containing the input (shape is 1 by 28 by 28)\n",
    "        labels: A 1d numpy array containing the labels (shape is 10)\n",
    "        params: A dictionary mapping parameter names to numpy arrays with the parameters.\n",
    "            This numpy array will contain W1, b1, W2 and b2\n",
    "            W1 and b1 represent the weights and bias for the hidden layer of the network\n",
    "            W2 and b2 represent the weights and bias for the output layer of the network\n",
    "\n",
    "    Returns:\n",
    "        A 2 element tuple containing:\n",
    "            1. A numpy array The output (after the softmax) of the output layer\n",
    "            2. The average loss for these data elements\n",
    "    \"\"\"\n",
    "\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "\n",
    "    first_convolution = forward_convolution(W1, b1, data)\n",
    "    first_max_pool = forward_max_pool(first_convolution, MAX_POOL_SIZE, MAX_POOL_SIZE)\n",
    "    first_after_relu = forward_relu(first_max_pool)\n",
    "\n",
    "    flattened = np.reshape(first_after_relu, (-1))\n",
    "\n",
    "    logits = forward_linear(W2, b2, flattened)\n",
    "\n",
    "    y = forward_softmax(logits)\n",
    "    cost = forward_cross_entropy_loss(y, labels)\n",
    "\n",
    "    return y, cost"
   ],
   "id": "b5b93f98e21eca71",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:08:47.933626Z",
     "start_time": "2025-09-03T15:08:47.915839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def backward_prop(data, labels, params):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation gradient computation step for a neural network\n",
    "\n",
    "    Args:\n",
    "        data: A numpy array containing the input for a single example\n",
    "        labels: A 1d numpy array containing the labels for a single example\n",
    "        params: A dictionary mapping parameter names to numpy arrays with the parameters.\n",
    "            This numpy array will contain W1, b1, W2, and b2\n",
    "            W1 and b1 represent the weights and bias for the convolutional layer\n",
    "            W2 and b2 represent the weights and bias for the output layer of the network\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of strings to numpy arrays where each key represents the name of a weight\n",
    "        and the values represent the gradient of the loss with respect to that weight.\n",
    "\n",
    "        In particular, it should have 4 elements:\n",
    "            W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    # 1. Get Parameters\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "\n",
    "    # 2. Forward to get cache value\n",
    "    first_convolution = forward_convolution(W1, b1, data)\n",
    "    first_max_pool = forward_max_pool(first_convolution, MAX_POOL_SIZE, MAX_POOL_SIZE)\n",
    "    first_after_relu = forward_relu(first_max_pool)\n",
    "    flattened = np.reshape(first_after_relu, (-1))\n",
    "    logits = forward_linear(W2, b2, flattened) # Input of Softmax\n",
    "    y = forward_softmax(logits)\n",
    "\n",
    "    # 3. Backward CE\n",
    "    grad_CE = backward_cross_entropy_loss(y, labels)   # dL/dy_hat\n",
    "\n",
    "    # 4. Backward Softmax\n",
    "    grad_sm = backward_softmax(logits, grad_CE)\n",
    "\n",
    "    # 4. Backward Linear\n",
    "    dW2, db2, dflattened = backward_linear(W2, b2, flattened, grad_sm)\n",
    "\n",
    "    # 5. Backward Flatten to ReLU\n",
    "    drelu = np.reshape(dflattened, first_after_relu.shape)\n",
    "    drelu_back = backward_relu(first_max_pool, drelu)\n",
    "\n",
    "    # 6. Backward Max Pooling\n",
    "    dmaxpool = backward_max_pool(first_convolution, MAX_POOL_SIZE, MAX_POOL_SIZE, drelu_back)\n",
    "\n",
    "    # 7. Backward Convolution\n",
    "    dW1, db1, _ = backward_convolution(W1, b1, data, dmaxpool)\n",
    "\n",
    "    # 8. Return Results\n",
    "    grads = {\n",
    "        'W1': dW1,\n",
    "        'b1': db1,\n",
    "        'W2': dW2,\n",
    "        'b2': db2\n",
    "    }\n",
    "\n",
    "    return grads\n",
    "    # *** END CODE HERE ***"
   ],
   "id": "a05ffdbff5f759d2",
   "outputs": [],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:08:47.996133Z",
     "start_time": "2025-09-03T15:08:47.987330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def forward_prop_batch(batch_data, batch_labels, params, forward_prop_func):\n",
    "    \"\"\"Apply the forward prop func to every image in a batch\"\"\"\n",
    "\n",
    "    y_array = []\n",
    "    cost_array = []\n",
    "\n",
    "    for item, label in zip(batch_data, batch_labels):\n",
    "        y, cost = forward_prop_func(item, label, params)\n",
    "        y_array.append(y)\n",
    "        cost_array.append(cost)\n",
    "\n",
    "    return np.array(y_array), np.array(cost_array)"
   ],
   "id": "3e56ee8e9eaac365",
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:08:48.065704Z",
     "start_time": "2025-09-03T15:08:48.052775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gradient_descent_batch(batch_data, batch_labels, learning_rate, params, backward_prop_func):\n",
    "    \"\"\"\n",
    "    Perform one batch of gradient descent on the given training data using the provided learning rate.\n",
    "\n",
    "    This code should update the parameters stored in params.\n",
    "    It should not return anything\n",
    "\n",
    "    Args:\n",
    "        batch_data: A numpy array containing the training data for the batch\n",
    "        train_labels: A numpy array containing the training labels for the batch\n",
    "        learning_rate: The learning rate\n",
    "        params: A dict of parameter names to parameter values that should be updated.\n",
    "        backward_prop_func: A function that follows the backwards_prop API\n",
    "\n",
    "    Returns: This function returns nothing.\n",
    "    \"\"\"\n",
    "\n",
    "    total_grad = {}\n",
    "\n",
    "    for i in range(batch_data.shape[0]):\n",
    "        grad = backward_prop_func(\n",
    "            batch_data[i, :, :],\n",
    "            batch_labels[i, :],\n",
    "            params)\n",
    "        for key, value in grad.items():\n",
    "            if key not in total_grad:\n",
    "                total_grad[key] = np.zeros(value.shape)\n",
    "\n",
    "            total_grad[key] += value\n",
    "\n",
    "    params['W1'] = params['W1'] - learning_rate * total_grad['W1']\n",
    "    params['W2'] = params['W2'] - learning_rate * total_grad['W2']\n",
    "    params['b1'] = params['b1'] - learning_rate * total_grad['b1']\n",
    "    params['b2'] = params['b2'] - learning_rate * total_grad['b2']\n",
    "\n",
    "    # This function does not return anything\n",
    "    return"
   ],
   "id": "cfc1ff9fb6435e14",
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model Training Algorithm",
   "id": "3c69235039698026"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:08:48.126866Z",
     "start_time": "2025-09-03T15:08:48.112795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def nn_train(\n",
    "    train_data, train_labels, dev_data, dev_labels,\n",
    "    get_initial_params_func, forward_prop_func, backward_prop_func,\n",
    "    learning_rate=5.0, batch_size=16, num_batches=400):\n",
    "\n",
    "    m = train_data.shape[0]\n",
    "\n",
    "    params = get_initial_params_func()\n",
    "\n",
    "    cost_dev = []\n",
    "    accuracy_dev = []\n",
    "    for batch in range(num_batches):\n",
    "        print('Currently processing {} / {}'.format(batch, num_batches))\n",
    "\n",
    "        batch_data = train_data[batch * batch_size:(batch + 1) * batch_size, :, :, :]\n",
    "        batch_labels = train_labels[batch * batch_size: (batch + 1) * batch_size, :]\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            output, cost = forward_prop_batch(dev_data, dev_labels, params, forward_prop_func)\n",
    "            cost_dev.append(sum(cost) / len(cost))\n",
    "            accuracy_dev.append(compute_accuracy(output, dev_labels))\n",
    "\n",
    "            print('Cost and accuracy', cost_dev[-1], accuracy_dev[-1])\n",
    "\n",
    "        gradient_descent_batch(batch_data, batch_labels,\n",
    "            learning_rate, params, backward_prop_func)\n",
    "\n",
    "    return params, cost_dev, accuracy_dev"
   ],
   "id": "6b5e545fb980d250",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:08:48.182799Z",
     "start_time": "2025-09-03T15:08:48.176978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def nn_test(data, labels, params):\n",
    "    output, cost = forward_prop(data, labels, params)\n",
    "    accuracy = compute_accuracy(output, labels)\n",
    "    return accuracy"
   ],
   "id": "bdfceec213c624c6",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:08:48.238426Z",
     "start_time": "2025-09-03T15:08:48.231444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_accuracy(output, labels):\n",
    "    correct_output = np.argmax(output,axis=1)\n",
    "    correct_labels = np.argmax(labels,axis=1)\n",
    "\n",
    "    is_correct = [a == b for a,b in zip(correct_output, correct_labels)]\n",
    "\n",
    "    accuracy = sum(is_correct) * 1. / labels.shape[0]\n",
    "    return accuracy"
   ],
   "id": "b26f357572febc51",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:08:48.293473Z",
     "start_time": "2025-09-03T15:08:48.287680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def one_hot_labels(labels):\n",
    "    one_hot_labels = np.zeros((labels.size, 10))\n",
    "    one_hot_labels[np.arange(labels.size),labels.astype(int)] = 1\n",
    "    return one_hot_labels"
   ],
   "id": "66661881ff819c63",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:08:48.349069Z",
     "start_time": "2025-09-03T15:08:48.342780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_data(images_file, labels_file):\n",
    "    x = np.loadtxt(images_file, delimiter=',')\n",
    "    y = np.loadtxt(labels_file, delimiter=',')\n",
    "\n",
    "    x = np.reshape(x, (x.shape[0], 1, 28, 28))\n",
    "\n",
    "    return x, y"
   ],
   "id": "6dfbef4f4cb20814",
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:08:48.409811Z",
     "start_time": "2025-09-03T15:08:48.399920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_train(all_data, all_labels, backward_prop_func):\n",
    "    params, cost_dev, accuracy_dev = nn_train(\n",
    "        all_data['train'], all_labels['train'],\n",
    "        all_data['dev'], all_labels['dev'],\n",
    "        get_initial_params, forward_prop, backward_prop_func,\n",
    "        learning_rate=1e-2, batch_size=16, num_batches=400\n",
    "    )\n",
    "\n",
    "    t = np.arange(400 // 100)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "\n",
    "    ax1.plot(t, cost_dev, 'b')\n",
    "    ax1.set_xlabel('time')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.set_title('Training curve')\n",
    "\n",
    "    ax2.plot(t, accuracy_dev, 'b')\n",
    "    ax2.set_xlabel('time')\n",
    "    ax2.set_ylabel('accuracy')\n",
    "\n",
    "    fig.savefig('output/train.png')"
   ],
   "id": "fa758699cdded78d",
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training Model",
   "id": "39fb3b09a301e4cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:08:48.467143Z",
     "start_time": "2025-09-03T15:08:48.456707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    np.random.seed(100)\n",
    "    train_data, train_labels = read_data('/home/anhnt02/Desktop/data_1/images_train.csv', '/home/anhnt02/Desktop/data_1/labels_train.csv')\n",
    "    train_labels = one_hot_labels(train_labels)\n",
    "    p = np.random.permutation(60000)\n",
    "    train_data = train_data[p,:]\n",
    "    train_labels = train_labels[p,:]\n",
    "\n",
    "    dev_data = train_data[0:400,:]\n",
    "    dev_labels = train_labels[0:400,:]\n",
    "    train_data = train_data[400:,:]\n",
    "    train_labels = train_labels[400:,:]\n",
    "\n",
    "    mean = np.mean(train_data)\n",
    "    std = np.std(train_data)\n",
    "    train_data = (train_data - mean) / std\n",
    "    dev_data = (dev_data - mean) / std\n",
    "\n",
    "    all_data = {\n",
    "        'train': train_data,\n",
    "        'dev': dev_data,\n",
    "    }\n",
    "\n",
    "    all_labels = {\n",
    "        'train': train_labels,\n",
    "        'dev': dev_labels,\n",
    "    }\n",
    "\n",
    "    run_train(all_data, all_labels, backward_prop)"
   ],
   "id": "a7bdf0c15e95a315",
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T15:16:09.749776Z",
     "start_time": "2025-09-03T15:08:48.528343Z"
    }
   },
   "cell_type": "code",
   "source": "main()",
   "id": "b435cc7284f462d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing 0 / 400\n",
      "Cost and accuracy 2.721417647426753 0.0725\n",
      "Currently processing 1 / 400\n",
      "Currently processing 2 / 400\n",
      "Currently processing 3 / 400\n",
      "Currently processing 4 / 400\n",
      "Currently processing 5 / 400\n",
      "Currently processing 6 / 400\n",
      "Currently processing 7 / 400\n",
      "Currently processing 8 / 400\n",
      "Currently processing 9 / 400\n",
      "Currently processing 10 / 400\n",
      "Currently processing 11 / 400\n",
      "Currently processing 12 / 400\n",
      "Currently processing 13 / 400\n",
      "Currently processing 14 / 400\n",
      "Currently processing 15 / 400\n",
      "Currently processing 16 / 400\n",
      "Currently processing 17 / 400\n",
      "Currently processing 18 / 400\n",
      "Currently processing 19 / 400\n",
      "Currently processing 20 / 400\n",
      "Currently processing 21 / 400\n",
      "Currently processing 22 / 400\n",
      "Currently processing 23 / 400\n",
      "Currently processing 24 / 400\n",
      "Currently processing 25 / 400\n",
      "Currently processing 26 / 400\n",
      "Currently processing 27 / 400\n",
      "Currently processing 28 / 400\n",
      "Currently processing 29 / 400\n",
      "Currently processing 30 / 400\n",
      "Currently processing 31 / 400\n",
      "Currently processing 32 / 400\n",
      "Currently processing 33 / 400\n",
      "Currently processing 34 / 400\n",
      "Currently processing 35 / 400\n",
      "Currently processing 36 / 400\n",
      "Currently processing 37 / 400\n",
      "Currently processing 38 / 400\n",
      "Currently processing 39 / 400\n",
      "Currently processing 40 / 400\n",
      "Currently processing 41 / 400\n",
      "Currently processing 42 / 400\n",
      "Currently processing 43 / 400\n",
      "Currently processing 44 / 400\n",
      "Currently processing 45 / 400\n",
      "Currently processing 46 / 400\n",
      "Currently processing 47 / 400\n",
      "Currently processing 48 / 400\n",
      "Currently processing 49 / 400\n",
      "Currently processing 50 / 400\n",
      "Currently processing 51 / 400\n",
      "Currently processing 52 / 400\n",
      "Currently processing 53 / 400\n",
      "Currently processing 54 / 400\n",
      "Currently processing 55 / 400\n",
      "Currently processing 56 / 400\n",
      "Currently processing 57 / 400\n",
      "Currently processing 58 / 400\n",
      "Currently processing 59 / 400\n",
      "Currently processing 60 / 400\n",
      "Currently processing 61 / 400\n",
      "Currently processing 62 / 400\n",
      "Currently processing 63 / 400\n",
      "Currently processing 64 / 400\n",
      "Currently processing 65 / 400\n",
      "Currently processing 66 / 400\n",
      "Currently processing 67 / 400\n",
      "Currently processing 68 / 400\n",
      "Currently processing 69 / 400\n",
      "Currently processing 70 / 400\n",
      "Currently processing 71 / 400\n",
      "Currently processing 72 / 400\n",
      "Currently processing 73 / 400\n",
      "Currently processing 74 / 400\n",
      "Currently processing 75 / 400\n",
      "Currently processing 76 / 400\n",
      "Currently processing 77 / 400\n",
      "Currently processing 78 / 400\n",
      "Currently processing 79 / 400\n",
      "Currently processing 80 / 400\n",
      "Currently processing 81 / 400\n",
      "Currently processing 82 / 400\n",
      "Currently processing 83 / 400\n",
      "Currently processing 84 / 400\n",
      "Currently processing 85 / 400\n",
      "Currently processing 86 / 400\n",
      "Currently processing 87 / 400\n",
      "Currently processing 88 / 400\n",
      "Currently processing 89 / 400\n",
      "Currently processing 90 / 400\n",
      "Currently processing 91 / 400\n",
      "Currently processing 92 / 400\n",
      "Currently processing 93 / 400\n",
      "Currently processing 94 / 400\n",
      "Currently processing 95 / 400\n",
      "Currently processing 96 / 400\n",
      "Currently processing 97 / 400\n",
      "Currently processing 98 / 400\n",
      "Currently processing 99 / 400\n",
      "Currently processing 100 / 400\n",
      "Cost and accuracy 0.6413721697623074 0.78\n",
      "Currently processing 101 / 400\n",
      "Currently processing 102 / 400\n",
      "Currently processing 103 / 400\n",
      "Currently processing 104 / 400\n",
      "Currently processing 105 / 400\n",
      "Currently processing 106 / 400\n",
      "Currently processing 107 / 400\n",
      "Currently processing 108 / 400\n",
      "Currently processing 109 / 400\n",
      "Currently processing 110 / 400\n",
      "Currently processing 111 / 400\n",
      "Currently processing 112 / 400\n",
      "Currently processing 113 / 400\n",
      "Currently processing 114 / 400\n",
      "Currently processing 115 / 400\n",
      "Currently processing 116 / 400\n",
      "Currently processing 117 / 400\n",
      "Currently processing 118 / 400\n",
      "Currently processing 119 / 400\n",
      "Currently processing 120 / 400\n",
      "Currently processing 121 / 400\n",
      "Currently processing 122 / 400\n",
      "Currently processing 123 / 400\n",
      "Currently processing 124 / 400\n",
      "Currently processing 125 / 400\n",
      "Currently processing 126 / 400\n",
      "Currently processing 127 / 400\n",
      "Currently processing 128 / 400\n",
      "Currently processing 129 / 400\n",
      "Currently processing 130 / 400\n",
      "Currently processing 131 / 400\n",
      "Currently processing 132 / 400\n",
      "Currently processing 133 / 400\n",
      "Currently processing 134 / 400\n",
      "Currently processing 135 / 400\n",
      "Currently processing 136 / 400\n",
      "Currently processing 137 / 400\n",
      "Currently processing 138 / 400\n",
      "Currently processing 139 / 400\n",
      "Currently processing 140 / 400\n",
      "Currently processing 141 / 400\n",
      "Currently processing 142 / 400\n",
      "Currently processing 143 / 400\n",
      "Currently processing 144 / 400\n",
      "Currently processing 145 / 400\n",
      "Currently processing 146 / 400\n",
      "Currently processing 147 / 400\n",
      "Currently processing 148 / 400\n",
      "Currently processing 149 / 400\n",
      "Currently processing 150 / 400\n",
      "Currently processing 151 / 400\n",
      "Currently processing 152 / 400\n",
      "Currently processing 153 / 400\n",
      "Currently processing 154 / 400\n",
      "Currently processing 155 / 400\n",
      "Currently processing 156 / 400\n",
      "Currently processing 157 / 400\n",
      "Currently processing 158 / 400\n",
      "Currently processing 159 / 400\n",
      "Currently processing 160 / 400\n",
      "Currently processing 161 / 400\n",
      "Currently processing 162 / 400\n",
      "Currently processing 163 / 400\n",
      "Currently processing 164 / 400\n",
      "Currently processing 165 / 400\n",
      "Currently processing 166 / 400\n",
      "Currently processing 167 / 400\n",
      "Currently processing 168 / 400\n",
      "Currently processing 169 / 400\n",
      "Currently processing 170 / 400\n",
      "Currently processing 171 / 400\n",
      "Currently processing 172 / 400\n",
      "Currently processing 173 / 400\n",
      "Currently processing 174 / 400\n",
      "Currently processing 175 / 400\n",
      "Currently processing 176 / 400\n",
      "Currently processing 177 / 400\n",
      "Currently processing 178 / 400\n",
      "Currently processing 179 / 400\n",
      "Currently processing 180 / 400\n",
      "Currently processing 181 / 400\n",
      "Currently processing 182 / 400\n",
      "Currently processing 183 / 400\n",
      "Currently processing 184 / 400\n",
      "Currently processing 185 / 400\n",
      "Currently processing 186 / 400\n",
      "Currently processing 187 / 400\n",
      "Currently processing 188 / 400\n",
      "Currently processing 189 / 400\n",
      "Currently processing 190 / 400\n",
      "Currently processing 191 / 400\n",
      "Currently processing 192 / 400\n",
      "Currently processing 193 / 400\n",
      "Currently processing 194 / 400\n",
      "Currently processing 195 / 400\n",
      "Currently processing 196 / 400\n",
      "Currently processing 197 / 400\n",
      "Currently processing 198 / 400\n",
      "Currently processing 199 / 400\n",
      "Currently processing 200 / 400\n",
      "Cost and accuracy 0.43482822210982586 0.8625\n",
      "Currently processing 201 / 400\n",
      "Currently processing 202 / 400\n",
      "Currently processing 203 / 400\n",
      "Currently processing 204 / 400\n",
      "Currently processing 205 / 400\n",
      "Currently processing 206 / 400\n",
      "Currently processing 207 / 400\n",
      "Currently processing 208 / 400\n",
      "Currently processing 209 / 400\n",
      "Currently processing 210 / 400\n",
      "Currently processing 211 / 400\n",
      "Currently processing 212 / 400\n",
      "Currently processing 213 / 400\n",
      "Currently processing 214 / 400\n",
      "Currently processing 215 / 400\n",
      "Currently processing 216 / 400\n",
      "Currently processing 217 / 400\n",
      "Currently processing 218 / 400\n",
      "Currently processing 219 / 400\n",
      "Currently processing 220 / 400\n",
      "Currently processing 221 / 400\n",
      "Currently processing 222 / 400\n",
      "Currently processing 223 / 400\n",
      "Currently processing 224 / 400\n",
      "Currently processing 225 / 400\n",
      "Currently processing 226 / 400\n",
      "Currently processing 227 / 400\n",
      "Currently processing 228 / 400\n",
      "Currently processing 229 / 400\n",
      "Currently processing 230 / 400\n",
      "Currently processing 231 / 400\n",
      "Currently processing 232 / 400\n",
      "Currently processing 233 / 400\n",
      "Currently processing 234 / 400\n",
      "Currently processing 235 / 400\n",
      "Currently processing 236 / 400\n",
      "Currently processing 237 / 400\n",
      "Currently processing 238 / 400\n",
      "Currently processing 239 / 400\n",
      "Currently processing 240 / 400\n",
      "Currently processing 241 / 400\n",
      "Currently processing 242 / 400\n",
      "Currently processing 243 / 400\n",
      "Currently processing 244 / 400\n",
      "Currently processing 245 / 400\n",
      "Currently processing 246 / 400\n",
      "Currently processing 247 / 400\n",
      "Currently processing 248 / 400\n",
      "Currently processing 249 / 400\n",
      "Currently processing 250 / 400\n",
      "Currently processing 251 / 400\n",
      "Currently processing 252 / 400\n",
      "Currently processing 253 / 400\n",
      "Currently processing 254 / 400\n",
      "Currently processing 255 / 400\n",
      "Currently processing 256 / 400\n",
      "Currently processing 257 / 400\n",
      "Currently processing 258 / 400\n",
      "Currently processing 259 / 400\n",
      "Currently processing 260 / 400\n",
      "Currently processing 261 / 400\n",
      "Currently processing 262 / 400\n",
      "Currently processing 263 / 400\n",
      "Currently processing 264 / 400\n",
      "Currently processing 265 / 400\n",
      "Currently processing 266 / 400\n",
      "Currently processing 267 / 400\n",
      "Currently processing 268 / 400\n",
      "Currently processing 269 / 400\n",
      "Currently processing 270 / 400\n",
      "Currently processing 271 / 400\n",
      "Currently processing 272 / 400\n",
      "Currently processing 273 / 400\n",
      "Currently processing 274 / 400\n",
      "Currently processing 275 / 400\n",
      "Currently processing 276 / 400\n",
      "Currently processing 277 / 400\n",
      "Currently processing 278 / 400\n",
      "Currently processing 279 / 400\n",
      "Currently processing 280 / 400\n",
      "Currently processing 281 / 400\n",
      "Currently processing 282 / 400\n",
      "Currently processing 283 / 400\n",
      "Currently processing 284 / 400\n",
      "Currently processing 285 / 400\n",
      "Currently processing 286 / 400\n",
      "Currently processing 287 / 400\n",
      "Currently processing 288 / 400\n",
      "Currently processing 289 / 400\n",
      "Currently processing 290 / 400\n",
      "Currently processing 291 / 400\n",
      "Currently processing 292 / 400\n",
      "Currently processing 293 / 400\n",
      "Currently processing 294 / 400\n",
      "Currently processing 295 / 400\n",
      "Currently processing 296 / 400\n",
      "Currently processing 297 / 400\n",
      "Currently processing 298 / 400\n",
      "Currently processing 299 / 400\n",
      "Currently processing 300 / 400\n",
      "Cost and accuracy 0.36739952605342974 0.87\n",
      "Currently processing 301 / 400\n",
      "Currently processing 302 / 400\n",
      "Currently processing 303 / 400\n",
      "Currently processing 304 / 400\n",
      "Currently processing 305 / 400\n",
      "Currently processing 306 / 400\n",
      "Currently processing 307 / 400\n",
      "Currently processing 308 / 400\n",
      "Currently processing 309 / 400\n",
      "Currently processing 310 / 400\n",
      "Currently processing 311 / 400\n",
      "Currently processing 312 / 400\n",
      "Currently processing 313 / 400\n",
      "Currently processing 314 / 400\n",
      "Currently processing 315 / 400\n",
      "Currently processing 316 / 400\n",
      "Currently processing 317 / 400\n",
      "Currently processing 318 / 400\n",
      "Currently processing 319 / 400\n",
      "Currently processing 320 / 400\n",
      "Currently processing 321 / 400\n",
      "Currently processing 322 / 400\n",
      "Currently processing 323 / 400\n",
      "Currently processing 324 / 400\n",
      "Currently processing 325 / 400\n",
      "Currently processing 326 / 400\n",
      "Currently processing 327 / 400\n",
      "Currently processing 328 / 400\n",
      "Currently processing 329 / 400\n",
      "Currently processing 330 / 400\n",
      "Currently processing 331 / 400\n",
      "Currently processing 332 / 400\n",
      "Currently processing 333 / 400\n",
      "Currently processing 334 / 400\n",
      "Currently processing 335 / 400\n",
      "Currently processing 336 / 400\n",
      "Currently processing 337 / 400\n",
      "Currently processing 338 / 400\n",
      "Currently processing 339 / 400\n",
      "Currently processing 340 / 400\n",
      "Currently processing 341 / 400\n",
      "Currently processing 342 / 400\n",
      "Currently processing 343 / 400\n",
      "Currently processing 344 / 400\n",
      "Currently processing 345 / 400\n",
      "Currently processing 346 / 400\n",
      "Currently processing 347 / 400\n",
      "Currently processing 348 / 400\n",
      "Currently processing 349 / 400\n",
      "Currently processing 350 / 400\n",
      "Currently processing 351 / 400\n",
      "Currently processing 352 / 400\n",
      "Currently processing 353 / 400\n",
      "Currently processing 354 / 400\n",
      "Currently processing 355 / 400\n",
      "Currently processing 356 / 400\n",
      "Currently processing 357 / 400\n",
      "Currently processing 358 / 400\n",
      "Currently processing 359 / 400\n",
      "Currently processing 360 / 400\n",
      "Currently processing 361 / 400\n",
      "Currently processing 362 / 400\n",
      "Currently processing 363 / 400\n",
      "Currently processing 364 / 400\n",
      "Currently processing 365 / 400\n",
      "Currently processing 366 / 400\n",
      "Currently processing 367 / 400\n",
      "Currently processing 368 / 400\n",
      "Currently processing 369 / 400\n",
      "Currently processing 370 / 400\n",
      "Currently processing 371 / 400\n",
      "Currently processing 372 / 400\n",
      "Currently processing 373 / 400\n",
      "Currently processing 374 / 400\n",
      "Currently processing 375 / 400\n",
      "Currently processing 376 / 400\n",
      "Currently processing 377 / 400\n",
      "Currently processing 378 / 400\n",
      "Currently processing 379 / 400\n",
      "Currently processing 380 / 400\n",
      "Currently processing 381 / 400\n",
      "Currently processing 382 / 400\n",
      "Currently processing 383 / 400\n",
      "Currently processing 384 / 400\n",
      "Currently processing 385 / 400\n",
      "Currently processing 386 / 400\n",
      "Currently processing 387 / 400\n",
      "Currently processing 388 / 400\n",
      "Currently processing 389 / 400\n",
      "Currently processing 390 / 400\n",
      "Currently processing 391 / 400\n",
      "Currently processing 392 / 400\n",
      "Currently processing 393 / 400\n",
      "Currently processing 394 / 400\n",
      "Currently processing 395 / 400\n",
      "Currently processing 396 / 400\n",
      "Currently processing 397 / 400\n",
      "Currently processing 398 / 400\n",
      "Currently processing 399 / 400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 108
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
