{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "# PS4-1: Neural Networks: MNIST Image Classification",
   "id": "5ef2d0779a09a5b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Library",
   "id": "96393687726f3b9c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T12:48:56.667292Z",
     "start_time": "2025-09-03T12:48:56.663258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ],
   "id": "af4ac1e784f2cb92",
   "outputs": [],
   "execution_count": 327
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Global Parameters",
   "id": "30c2bd9c4dc33a34"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T12:48:56.684535Z",
     "start_time": "2025-09-03T12:48:56.680483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_POOL_SIZE = 5\n",
    "CONVOLUTION_SIZE = 4\n",
    "CONVOLUTION_FILTERS = 2"
   ],
   "id": "573b1d5abad9eb94",
   "outputs": [],
   "execution_count": 328
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training NN Algorithm",
   "id": "3e0bc68626bbc8b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model Architcture",
   "id": "f73616598932fb8b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T12:48:56.739080Z",
     "start_time": "2025-09-03T12:48:56.733164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def forward_softmax(x):\n",
    "    \"\"\"\n",
    "    Compute softmax function for a single example.\n",
    "    The shape of the input is of size # num classes.\n",
    "\n",
    "    Important Note: You must be careful to avoid overflow for this function. Functions\n",
    "    like softmax have a tendency to overflow when very large numbers like e^10000 are computed.\n",
    "    You will know that your function is overflow resistent when it can handle input like:\n",
    "    np.array([[10000, 10010, 10]]) without issues.\n",
    "\n",
    "        x: A 1d numpy float array of shape number_of_classes\n",
    "\n",
    "    Returns:\n",
    "        A 1d numpy float array containing the softmax results of shape  number_of_classes\n",
    "    \"\"\"\n",
    "    x = x - np.max(x,axis=0)\n",
    "    exp = np.exp(x)\n",
    "    s = exp / np.sum(exp,axis=0)\n",
    "    return s"
   ],
   "id": "ec594d5c6f7ddd9b",
   "outputs": [],
   "execution_count": 329
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T12:48:56.797453Z",
     "start_time": "2025-09-03T12:48:56.788411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def backward_softmax(x, grad_outputs):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with respect to x.\n",
    "\n",
    "    grad_outputs is the gradient of the loss with respect to the outputs of the softmax.\n",
    "\n",
    "    Args:\n",
    "        x: A 1d numpy float array of shape number_of_classes\n",
    "        grad_outputs: A 1d numpy float array of shape number_of_classes\n",
    "\n",
    "    Returns:\n",
    "        A 1d numpy float array of the same shape as x with the derivative of the loss with respect to x\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    \"\"\"\n",
    "    We need to compute derivative of CE loss w.r.t x\n",
    "    Note: x = logits (input of softmax) => x is 1D, y_hat = softmax(x)\n",
    "    Formula: dCE/dx = dCE/dy_hat * dy_hat/dx (Chain Rule), where:\n",
    "        dCE/dy_hat = grad_outputs\n",
    "        dy_hat/dx = Derivative of y_hat w.r.t x\n",
    "    \"\"\"\n",
    "    # 1. Get x size = number_of_classes\n",
    "    K = len(x)\n",
    "\n",
    "    # 2. Calculate Softmax\n",
    "    y_hat = forward_softmax(x)\n",
    "\n",
    "    # 3. Update dCE/dx\n",
    "    d_CE_x = np.zeros(K)\n",
    "\n",
    "    for j in range(K):\n",
    "        for k in range(K):\n",
    "            if j == k:\n",
    "                d_CE_x[j] += grad_outputs[k] * y_hat[k] * (1 - y_hat[k])\n",
    "            else:\n",
    "                d_CE_x[j] += grad_outputs[k] * (- y_hat[k] * y_hat[j])\n",
    "\n",
    "    return d_CE_x\n",
    "    # *** END CODE HERE ***"
   ],
   "id": "ce0ddf35c0a83b01",
   "outputs": [],
   "execution_count": 330
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T12:48:56.851472Z",
     "start_time": "2025-09-03T12:48:56.846872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def forward_relu(x):\n",
    "    \"\"\"\n",
    "    Compute the relu function for the input x.\n",
    "\n",
    "    Args:\n",
    "        x: A numpy float array\n",
    "\n",
    "    Returns:\n",
    "        A numpy float array containing the relu results\n",
    "    \"\"\"\n",
    "    x[x<=0] = 0\n",
    "\n",
    "    return x"
   ],
   "id": "fddfbb65a14a33f1",
   "outputs": [],
   "execution_count": 331
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T12:48:56.909578Z",
     "start_time": "2025-09-03T12:48:56.900623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def backward_relu(x, grad_outputs):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with respect to x\n",
    "\n",
    "    Args:\n",
    "        x: A numpy array of arbitrary shape containing the input.\n",
    "        grad_outputs: A numpy array of the same shape of x containing the gradient of the loss with respect\n",
    "            to the output of relu\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of the same shape as x containing the gradients with respect to x.\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    \"\"\"\n",
    "    We need to compute derivative of \"Cross-Entropy Loss is calculated after Softmax\" loss w.r.t x\n",
    "    Note: x = MaxPooling output (input of ReLU) => x is 3D image (Batch Size, Channels, Heights, Widths)\n",
    "    Formula: dL/dx = dL/reLU * dreLU/dx (Chain Rule), where:\n",
    "        dL/dreLU = grad_outputs\n",
    "        dreLU/dx  = Derivative of reLU w.r.t x\n",
    "    \"\"\"\n",
    "    # 1. Get x shape\n",
    "    C, H, W = x.shape\n",
    "\n",
    "    # 2. Update dL/dreLU\n",
    "    grad_x = np.zeros(x.shape)\n",
    "    # for b in range(B):\n",
    "    for c in range(C):\n",
    "        for h in range(H):\n",
    "            for w in range(W):\n",
    "                if x[c, h, w] > 0:\n",
    "                    grad_x[c, h, w] = 1 * grad_outputs[c, h, w]\n",
    "                else:\n",
    "                    grad_x[c, h, w] = 0\n",
    "\n",
    "    return grad_x\n",
    "    # *** END CODE HERE ***"
   ],
   "id": "4da374176cb978bf",
   "outputs": [],
   "execution_count": 332
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T12:48:56.969080Z",
     "start_time": "2025-09-03T12:48:56.960404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_initial_params():\n",
    "    \"\"\"\n",
    "    Compute the initial parameters for the neural network.\n",
    "\n",
    "    This function should return a dictionary mapping parameter names to numpy arrays containing\n",
    "    the initial values for those parameters.\n",
    "\n",
    "    There should be four parameters for this model:\n",
    "    W1 is the weight matrix for the convolutional layer\n",
    "    b1 is the bias vector for the convolutional layer\n",
    "    W2 is the weight matrix for the output layers\n",
    "    b2 is the bias vector for the output layer\n",
    "\n",
    "    Weight matrices should be initialized with values drawn from a random normal distribution.\n",
    "    The mean of that distribution should be 0.\n",
    "    The variance of that distribution should be 1/sqrt(n) where n is the number of neurons that\n",
    "    feed into an output for that layer.\n",
    "\n",
    "    Bias vectors should be initialized with zero.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        A dict mapping parameter names to numpy arrays\n",
    "    \"\"\"\n",
    "\n",
    "    size_after_convolution = 28 - CONVOLUTION_SIZE + 1\n",
    "    size_after_max_pooling = size_after_convolution // MAX_POOL_SIZE\n",
    "\n",
    "    num_hidden = size_after_max_pooling * size_after_max_pooling * CONVOLUTION_FILTERS\n",
    "\n",
    "    return {\n",
    "        'W1': np.random.normal(size = (CONVOLUTION_FILTERS, 1, CONVOLUTION_SIZE, CONVOLUTION_SIZE), scale=1/ math.sqrt(CONVOLUTION_SIZE * CONVOLUTION_SIZE)),\n",
    "        'b1': np.zeros(CONVOLUTION_FILTERS),\n",
    "        'W2': np.random.normal(size = (num_hidden, 10), scale = 1/ math.sqrt(num_hidden)),\n",
    "        'b2': np.zeros(10)\n",
    "    }"
   ],
   "id": "a2eb67b0070ee17d",
   "outputs": [],
   "execution_count": 333
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T12:48:57.029047Z",
     "start_time": "2025-09-03T12:48:57.018555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def forward_convolution(conv_W, conv_b, data):\n",
    "    \"\"\"\n",
    "    Compute the output from a convolutional layer given the weights and data.\n",
    "\n",
    "    conv_W is of the shape (# output channels, # input channels, convolution width, convolution height )\n",
    "    conv_b is of the shape (# output channels)\n",
    "\n",
    "    data is of the shape (# input channels, width, height)\n",
    "\n",
    "    The output should be the result of a convolution and should be of the size:\n",
    "        (# output channels, width - convolution width + 1, height -  convolution height + 1)\n",
    "\n",
    "    Returns:\n",
    "        The output of the convolution as a numpy array\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    oc = output_channels\n",
    "    ic = input_channels\n",
    "    x = starting coordinates in width.\n",
    "    y = starting coordinates in height.\n",
    "    di = conv_W_offset_width\n",
    "    dj = conv_W_offset_height\n",
    "    data = input data\n",
    "\n",
    "    Formula:\n",
    "    output[oc, x, y] = conv_b[oc] + sum_di,sum_dj,sum_ic (data[ic, x + di, y + dj] * conv_W[oc, ic, di, dj])\n",
    "    \"\"\"\n",
    "    conv_channels, _, conv_width, conv_height = conv_W.shape\n",
    "\n",
    "    input_channels, input_width, input_height = data.shape\n",
    "\n",
    "    output = np.zeros((conv_channels, input_width - conv_width + 1, input_height - conv_height + 1))\n",
    "\n",
    "    for x in range(input_width - conv_width + 1):\n",
    "        for y in range(input_height - conv_height + 1):\n",
    "            for output_channel in range(conv_channels):\n",
    "                output[output_channel, x, y] = np.sum(\n",
    "                    np.multiply(data[:, x:(x + conv_width), y:(y + conv_height)], conv_W[output_channel, :, :, :])) + conv_b[output_channel]\n",
    "\n",
    "    return output"
   ],
   "id": "35700cc30c212df1",
   "outputs": [],
   "execution_count": 334
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T12:48:57.097458Z",
     "start_time": "2025-09-03T12:48:57.078843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def backward_convolution(conv_W, conv_b, data, output_grad):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with respect to the parameters of the convolution.\n",
    "\n",
    "    See forward_convolution for the sizes of the arguments.\n",
    "    output_grad is the gradient of the loss with respect to the output of the convolution.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing 3 gradients.\n",
    "        The first element is the gradient of the loss with respect to the convolution weights\n",
    "        The second element is the gradient of the loss with respect to the convolution bias\n",
    "        The third element is the gradient of the loss with respect to the input data\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    \"\"\"\n",
    "    We need to compute derivative of \"Loss Function\" loss w.r.t conv_W, conv_b, data in tuple\n",
    "\n",
    "    Parameters:\n",
    "    oc = output_channels\n",
    "    ic = input_channels\n",
    "    x = starting coordinates in width (data)\n",
    "    y = starting coordinates in height (data)\n",
    "    di = conv_W_offset_width\n",
    "    dj = conv_W_offset_height\n",
    "    data = input data\n",
    "\n",
    "    Formula:\n",
    "    dL/doutput = dL/doutput[oc, x, y] === output_grad\n",
    "\n",
    "    doutput/dconv_W = doutput[oc, x, y]/dconv_W[oc, ic, di, dj] === data[ic, x + di, y + dj]\n",
    "\n",
    "    doutput/dconv_b = doutput[oc, x, y]/dconv_b[oc] === 1\n",
    "\n",
    "    doutput/ddata = doutput[oc, x, y]/ddata[ic, x+di, y+dj] === conv_W[oc, ic, di, dj]\n",
    "\n",
    "    So, we have full Formula:\n",
    "    1. dL/dconv_W = dL/doutput * doutput/dconv_W === sum_x,sum_y (output_grad[oc, x, y] * data[ic, x + di, y + dj])\n",
    "\n",
    "    2. dL/dconv_b = dL/doutput * doutput/dconv_b === sum_x, sum_y (output_grad[oc, x, y] * 1)\n",
    "\n",
    "    3. dL/ddata = dL/doutput * doutput/ddata === sum_oc, sum_di, sum_dj (output_grad[oc, x-di, y-dj] * conv_W[oc, ic, di, dj])\n",
    "    \"\"\"\n",
    "    # 1. Get Parameters\n",
    "    conv_channels, _, conv_width, conv_height = conv_W.shape\n",
    "    input_channels, input_width, input_height = data.shape\n",
    "    _, output_width, output_height = output_grad.shape\n",
    "\n",
    "    # 2. Initialize Gradient\n",
    "    dW = np.zeros(conv_W.shape)\n",
    "    db = np.zeros(conv_b.shape)\n",
    "    ddata = np.zeros(data.shape)\n",
    "\n",
    "    # 3. dL/dconv_W\n",
    "    for oc in range(conv_channels):\n",
    "        for ic in range(input_channels):\n",
    "            for di in range(conv_width):\n",
    "                for dj in range(conv_height):\n",
    "                    temp_grad = 0\n",
    "                    for x in range(output_width):\n",
    "                        for y in range(output_height):\n",
    "                            temp_grad += output_grad[oc, x, y] * data[ic, x + di, y + dj]\n",
    "                    dW[oc, ic, di, dj] = temp_grad\n",
    "\n",
    "    # 4. dL/dconv_b\n",
    "    for oc in range(conv_channels):\n",
    "        temp_grad = 0\n",
    "        for x in range(output_width):\n",
    "            for y in range(output_height):\n",
    "                temp_grad += output_grad[oc, x, y] * 1\n",
    "        db[oc] = temp_grad\n",
    "\n",
    "    # 5. dL/ddata\n",
    "    for oc in range(conv_channels):\n",
    "        for ic in range(input_channels):\n",
    "            for x in range(input_width):\n",
    "                for y in range(input_height):\n",
    "                    temp_grad = 0\n",
    "                    for di in range(conv_width):\n",
    "                        for dj in range(conv_height):\n",
    "                            x_out = x - di\n",
    "                            y_out = y - dj\n",
    "                            if 0 <= x_out < output_width and 0 <= y_out < output_height:\n",
    "                                temp_grad += output_grad[oc, x_out, y_out] * conv_W[oc, ic, di, dj]\n",
    "                    ddata[ic, x, y] = temp_grad\n",
    "\n",
    "    return dW, db, ddata\n",
    "    # *** END CODE HERE ***"
   ],
   "id": "c15417e8b8c81c93",
   "outputs": [],
   "execution_count": 335
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T12:48:57.155966Z",
     "start_time": "2025-09-03T12:48:57.146705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def forward_max_pool(data, pool_width, pool_height):\n",
    "    \"\"\"\n",
    "    Compute the output from a max pooling layer given the data and pool dimensions.\n",
    "\n",
    "    The stride length should be equal to the pool size\n",
    "\n",
    "    data is of the shape (# channels, width, height)\n",
    "\n",
    "    The output should be the result of the max pooling layer and should be of size:\n",
    "        (# channels, width // pool_width, height // pool_height)\n",
    "\n",
    "    Returns:\n",
    "        The result of the max pooling layer\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    oc = output_channels\n",
    "    ic = input_channels\n",
    "    x = starting coordinates in width (data)\n",
    "    y = starting coordinates in height (data)\n",
    "    di = conv_W_offset_width\n",
    "    dj = conv_W_offset_height\n",
    "    data = input data\n",
    "\n",
    "    Formula:\n",
    "    output[oc, x, y] = max_di,dj (data[ic, x * pool_width + di, y * pool_height + dj])\n",
    "    \"\"\"\n",
    "    input_channels, input_width, input_height = data.shape\n",
    "\n",
    "    output = np.zeros((input_channels, input_width // pool_width, input_height // pool_height))\n",
    "\n",
    "    for x in range(0, input_width, pool_width):\n",
    "        for y in range(0, input_height, pool_height):\n",
    "\n",
    "            output[:, x // pool_width, y // pool_height] = np.amax(data[:, x:(x + pool_width), y:(y + pool_height)], axis=(1, 2))\n",
    "\n",
    "    return output"
   ],
   "id": "d913c8ec5222e122",
   "outputs": [],
   "execution_count": 336
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T12:48:57.219387Z",
     "start_time": "2025-09-03T12:48:57.207026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def backward_max_pool(data, pool_width, pool_height, output_grad):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with respect to the data in the max pooling layer.\n",
    "\n",
    "    data is of the shape (# channels, width, height)\n",
    "    output_grad is of shape (# channels, width // pool_width, height // pool_height)\n",
    "\n",
    "    output_grad is the gradient of the loss with respect to the output of the backward max\n",
    "    pool layer.\n",
    "\n",
    "    Returns:\n",
    "        The gradient of the loss with respect to the data (of same shape as data)\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    \"\"\"\n",
    "    We need to compute derivative of \"Loss Function\" loss w.r.t data\n",
    "\n",
    "    Parameters:\n",
    "    oc = output_channels\n",
    "    ic = input_channels\n",
    "    x = starting coordinates in width (data)\n",
    "    y = starting coordinates in height (data)\n",
    "    di = step of pool_width\n",
    "    dj = step of pool_height\n",
    "    data = input data\n",
    "    x_out = x/pool_width\n",
    "    y_out = y/pool_height\n",
    "\n",
    "    Formula:\n",
    "    dL/doutput = dL/doutput[oc, x, y] === output_grad[oc, x_out, y_out]\n",
    "\n",
    "    doutput/ddata = doutput[oc, x, y]/ddata[ic, x * pool_width + di, y * pool_height + dj] === 1 (if (i,j) is max in (x_out, y_out)) else === 0\n",
    "\n",
    "    So, we have ful Formula:\n",
    "    dL/ddata = dL/doutput * doutput/ddata === output_grad[oc, x_out, y_out] * 1 (if (i,j) is max in (x_out, y_out)) or === 0\n",
    "    \"\"\"\n",
    "    # 1. Get Parameters\n",
    "    input_channels, input_width, input_height = data.shape\n",
    "    X_out, Y_out = input_width // pool_width, input_height // pool_height\n",
    "    _, output_width, output_height = output_grad.shape\n",
    "\n",
    "    # 2. Initialize Parameters\n",
    "    ddata = np.zeros(data.shape)\n",
    "\n",
    "    # 3. dL/ddata\n",
    "    for ic in range(input_channels):\n",
    "        for x_out in range(X_out):\n",
    "            for y_out in range(Y_out):\n",
    "                max_i, max_j = 0, 0\n",
    "                max_value = -1e9\n",
    "                for di in range(pool_width):\n",
    "                    for dj in range(pool_height):\n",
    "                        val = data[ic, x_out * pool_width + di, y_out * pool_height + dj]\n",
    "                        if val > max_value:\n",
    "                            max_value = 1 * val\n",
    "                            max_i, max_j = di, dj\n",
    "                ddata[ic, x_out * pool_width + max_i, y_out * pool_height + max_j] = output_grad[ic, x_out, y_out]          #oc = ic\n",
    "\n",
    "    return ddata\n",
    "    # *** END CODE HERE ***"
   ],
   "id": "728e3e9a04a66846",
   "outputs": [],
   "execution_count": 337
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T12:48:57.277176Z",
     "start_time": "2025-09-03T12:48:57.269907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def forward_cross_entropy_loss(probabilities, labels):\n",
    "    \"\"\"\n",
    "    Compute the output from a cross entropy loss layer given the probabilities and labels.\n",
    "\n",
    "    probabilities is of the shape (# classes)\n",
    "    labels is of the shape (# classes)\n",
    "\n",
    "    The output should be a scalar\n",
    "\n",
    "    Returns:\n",
    "        The result of the log loss layer\n",
    "    \"\"\"\n",
    "\n",
    "    result = 0\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        if label == 1:\n",
    "            result += -np.log(probabilities[i])\n",
    "\n",
    "    return result"
   ],
   "id": "c242d0b702c5a9af",
   "outputs": [],
   "execution_count": 338
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T12:48:57.335797Z",
     "start_time": "2025-09-03T12:48:57.328460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def backward_cross_entropy_loss(probabilities, labels):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the cross entropy loss with respect to the probabilities.\n",
    "\n",
    "    probabilities is of the shape (# classes)\n",
    "    labels is of the shape (# classes)\n",
    "\n",
    "    The output should be the gradient with respect to the probabilities.\n",
    "\n",
    "    Returns:\n",
    "        The gradient of the loss with respect to the probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    \"\"\"\n",
    "    We need to estimate Derivative of CE Loss w.r.t y_hat_k (probabilities)\n",
    "\n",
    "    Formula:\n",
    "    dCE/dx_k = y_hat_k - y_k === probabilities[k] - labels[k]\n",
    "    \"\"\"\n",
    "    # 1. Get labels size\n",
    "    K = len(labels)\n",
    "\n",
    "    # 2. Initialize gradient\n",
    "    grad = np.zeros(K)\n",
    "\n",
    "    # 3. dCE/dx_k\n",
    "    for k in range(K):\n",
    "        if labels[k] == 1:\n",
    "            grad[k] = -1.0 / probabilities[k]\n",
    "        else:\n",
    "            grad[k] = 0.0\n",
    "\n",
    "    return grad\n",
    "    # *** END CODE HERE ***"
   ],
   "id": "722b1f825c764a28",
   "outputs": [],
   "execution_count": 339
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T12:48:57.392287Z",
     "start_time": "2025-09-03T12:48:57.385444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def forward_linear(weights, bias, data):\n",
    "    \"\"\"\n",
    "    Compute the output from a linear layer with the given weights, bias and data.\n",
    "    weights is of the shape (input # features, output # features)\n",
    "    bias is of the shape (output # features)\n",
    "    data is of the shape (input # features)\n",
    "\n",
    "    The output should be of the shape (output # features)\n",
    "\n",
    "    Returns:\n",
    "        The result of the linear layer\n",
    "    \"\"\"\n",
    "    return data.dot(weights) + bias"
   ],
   "id": "9f94740e7ed9370b",
   "outputs": [],
   "execution_count": 340
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T12:48:57.452482Z",
     "start_time": "2025-09-03T12:48:57.440664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def backward_linear(weights, bias, data, output_grad):\n",
    "    \"\"\"\n",
    "    Compute the gradients of the loss with respect to the parameters of a linear layer.\n",
    "\n",
    "    See forward_linear for information about the shapes of the variables.\n",
    "\n",
    "    output_grad is the gradient of the loss with respect to the output of this layer.\n",
    "\n",
    "    This should return a tuple with three elements:\n",
    "    - The gradient of the loss with respect to the weights\n",
    "    - The gradient of the loss with respect to the bias\n",
    "    - The gradient of the loss with respect to the data\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    \"\"\"\n",
    "    We need to estimate \"Loss Function\" w.r.t weights, bias, data\n",
    "\n",
    "    Parameters:\n",
    "    weights = 2D (input_features, output_features)\n",
    "    bias = 1D (output_features)\n",
    "    data = 1D (input_features)\n",
    "\n",
    "    Formula:\n",
    "    Linear = x.W + b\n",
    "\n",
    "    dL/dLinear = output_grad\n",
    "    dL/dweights = dL/dLinear * dLinear/dweights === output_grad * data\n",
    "    dL/dbias = dL/dLinear * dLinear/dbias === output_grad * 1\n",
    "    dL/ddata = dL/dLinear * dLinear/ddata === output_grad * weights\n",
    "    \"\"\"\n",
    "    # 1. Get weights shape\n",
    "    input_features, output_features = weights.shape\n",
    "\n",
    "    # 2. Initialize gradients\n",
    "    dweights = np.zeros(weights.shape)\n",
    "    dbias = np.zeros(bias.shape)\n",
    "    ddata = np.zeros(data.shape)\n",
    "\n",
    "    # 3. dweights[i,j] = output_grad[j] * data[i]\n",
    "    for i in range(input_features):\n",
    "        for j in range(output_features):\n",
    "            dweights[i, j] = output_grad[j] * data[i]\n",
    "\n",
    "    # 4. dbias[j] = output_grad[j] * 1\n",
    "    for j in range(output_features):\n",
    "        dbias[j] = output_grad[j]\n",
    "\n",
    "    # 5. ddata[i] = output_grad[j] * weights[i, j]\n",
    "    for i in range(input_features):\n",
    "        temp = 0\n",
    "        for j in range(output_features):\n",
    "            temp += output_grad[j] * weights[i, j]\n",
    "        ddata[i] = temp\n",
    "\n",
    "    return dweights, dbias, ddata\n",
    "    # *** END CODE HERE ***"
   ],
   "id": "e178a42eb20f4370",
   "outputs": [],
   "execution_count": 341
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T12:48:57.510531Z",
     "start_time": "2025-09-03T12:48:57.501417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def forward_prop(data, labels, params):\n",
    "    \"\"\"\n",
    "    Implement the forward layer given the data, labels, and params.\n",
    "\n",
    "    Args:\n",
    "        data: A numpy array containing the input (shape is 1 by 28 by 28)\n",
    "        labels: A 1d numpy array containing the labels (shape is 10)\n",
    "        params: A dictionary mapping parameter names to numpy arrays with the parameters.\n",
    "            This numpy array will contain W1, b1, W2 and b2\n",
    "            W1 and b1 represent the weights and bias for the hidden layer of the network\n",
    "            W2 and b2 represent the weights and bias for the output layer of the network\n",
    "\n",
    "    Returns:\n",
    "        A 2 element tuple containing:\n",
    "            1. A numpy array The output (after the softmax) of the output layer\n",
    "            2. The average loss for these data elements\n",
    "    \"\"\"\n",
    "\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "\n",
    "    first_convolution = forward_convolution(W1, b1, data)\n",
    "    first_max_pool = forward_max_pool(first_convolution, MAX_POOL_SIZE, MAX_POOL_SIZE)\n",
    "    first_after_relu = forward_relu(first_max_pool)\n",
    "\n",
    "    flattened = np.reshape(first_after_relu, (-1))\n",
    "\n",
    "    logits = forward_linear(W2, b2, flattened)\n",
    "\n",
    "    y = forward_softmax(logits)\n",
    "    cost = forward_cross_entropy_loss(y, labels)\n",
    "\n",
    "    return y, cost"
   ],
   "id": "b5b93f98e21eca71",
   "outputs": [],
   "execution_count": 342
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T12:48:57.573076Z",
     "start_time": "2025-09-03T12:48:57.560603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def backward_prop(data, labels, params):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation gradient computation step for a neural network\n",
    "\n",
    "    Args:\n",
    "        data: A numpy array containing the input for a single example\n",
    "        labels: A 1d numpy array containing the labels for a single example\n",
    "        params: A dictionary mapping parameter names to numpy arrays with the parameters.\n",
    "            This numpy array will contain W1, b1, W2, and b2\n",
    "            W1 and b1 represent the weights and bias for the convolutional layer\n",
    "            W2 and b2 represent the weights and bias for the output layer of the network\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of strings to numpy arrays where each key represents the name of a weight\n",
    "        and the values represent the gradient of the loss with respect to that weight.\n",
    "\n",
    "        In particular, it should have 4 elements:\n",
    "            W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    # 1. Get Parameters\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "\n",
    "    # 2. Forward to get cache value\n",
    "    first_convolution = forward_convolution(W1, b1, data)\n",
    "    first_max_pool = forward_max_pool(first_convolution, MAX_POOL_SIZE, MAX_POOL_SIZE)\n",
    "    first_after_relu = forward_relu(first_max_pool)\n",
    "    flattened = np.reshape(first_after_relu, (-1))\n",
    "    logits = forward_linear(W2, b2, flattened)\n",
    "    y = forward_softmax(logits)\n",
    "\n",
    "    # 3. Backward Softmax + CE\n",
    "    grad_logits = backward_cross_entropy_loss(y, labels)   # dL/dlogits\n",
    "\n",
    "    # 4. Backward Linear\n",
    "    dW2, db2, dflattened = backward_linear(W2, b2, flattened, grad_logits)\n",
    "\n",
    "    # 5. Backward Flatten to ReLU\n",
    "    drelu = np.reshape(dflattened, first_after_relu.shape)\n",
    "    drelu_back = backward_relu(first_max_pool, drelu)\n",
    "\n",
    "    # 6. Backward Max Pooling\n",
    "    dmaxpool = backward_max_pool(first_convolution, MAX_POOL_SIZE, MAX_POOL_SIZE, drelu_back)\n",
    "\n",
    "    # 7. Backward Convolution\n",
    "    dW1, db1, _ = backward_convolution(W1, b1, data, dmaxpool)\n",
    "\n",
    "    # 8. Return Results\n",
    "    grads = {\n",
    "        'W1': dW1,\n",
    "        'b1': db1,\n",
    "        'W2': dW2,\n",
    "        'b2': db2\n",
    "    }\n",
    "\n",
    "    return grads\n",
    "    # *** END CODE HERE ***"
   ],
   "id": "a05ffdbff5f759d2",
   "outputs": [],
   "execution_count": 343
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T12:48:57.629816Z",
     "start_time": "2025-09-03T12:48:57.622857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def forward_prop_batch(batch_data, batch_labels, params, forward_prop_func):\n",
    "    \"\"\"Apply the forward prop func to every image in a batch\"\"\"\n",
    "\n",
    "    y_array = []\n",
    "    cost_array = []\n",
    "\n",
    "    for item, label in zip(batch_data, batch_labels):\n",
    "        y, cost = forward_prop_func(item, label, params)\n",
    "        y_array.append(y)\n",
    "        cost_array.append(cost)\n",
    "\n",
    "    return np.array(y_array), np.array(cost_array)"
   ],
   "id": "3e56ee8e9eaac365",
   "outputs": [],
   "execution_count": 344
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T12:48:57.690315Z",
     "start_time": "2025-09-03T12:48:57.678842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gradient_descent_batch(batch_data, batch_labels, learning_rate, params, backward_prop_func):\n",
    "    \"\"\"\n",
    "    Perform one batch of gradient descent on the given training data using the provided learning rate.\n",
    "\n",
    "    This code should update the parameters stored in params.\n",
    "    It should not return anything\n",
    "\n",
    "    Args:\n",
    "        batch_data: A numpy array containing the training data for the batch\n",
    "        train_labels: A numpy array containing the training labels for the batch\n",
    "        learning_rate: The learning rate\n",
    "        params: A dict of parameter names to parameter values that should be updated.\n",
    "        backward_prop_func: A function that follows the backwards_prop API\n",
    "\n",
    "    Returns: This function returns nothing.\n",
    "    \"\"\"\n",
    "\n",
    "    total_grad = {}\n",
    "\n",
    "    for i in range(batch_data.shape[0]):\n",
    "        grad = backward_prop_func(\n",
    "            batch_data[i, :, :],\n",
    "            batch_labels[i, :],\n",
    "            params)\n",
    "        for key, value in grad.items():\n",
    "            if key not in total_grad:\n",
    "                total_grad[key] = np.zeros(value.shape)\n",
    "\n",
    "            total_grad[key] += value\n",
    "\n",
    "    params['W1'] = params['W1'] - learning_rate * total_grad['W1']\n",
    "    params['W2'] = params['W2'] - learning_rate * total_grad['W2']\n",
    "    params['b1'] = params['b1'] - learning_rate * total_grad['b1']\n",
    "    params['b2'] = params['b2'] - learning_rate * total_grad['b2']\n",
    "\n",
    "    # This function does not return anything\n",
    "    return"
   ],
   "id": "cfc1ff9fb6435e14",
   "outputs": [],
   "execution_count": 345
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model Training Algorithm",
   "id": "3c69235039698026"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T12:48:57.747208Z",
     "start_time": "2025-09-03T12:48:57.735284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def nn_train(\n",
    "    train_data, train_labels, dev_data, dev_labels,\n",
    "    get_initial_params_func, forward_prop_func, backward_prop_func,\n",
    "    learning_rate=5.0, batch_size=16, num_batches=400):\n",
    "\n",
    "    m = train_data.shape[0]\n",
    "\n",
    "    params = get_initial_params_func()\n",
    "\n",
    "    cost_dev = []\n",
    "    accuracy_dev = []\n",
    "    for batch in range(num_batches):\n",
    "        print('Currently processing {} / {}'.format(batch, num_batches))\n",
    "\n",
    "        batch_data = train_data[batch * batch_size:(batch + 1) * batch_size, :, :, :]\n",
    "        batch_labels = train_labels[batch * batch_size: (batch + 1) * batch_size, :]\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            output, cost = forward_prop_batch(dev_data, dev_labels, params, forward_prop_func)\n",
    "            cost_dev.append(sum(cost) / len(cost))\n",
    "            accuracy_dev.append(compute_accuracy(output, dev_labels))\n",
    "\n",
    "            print('Cost and accuracy', cost_dev[-1], accuracy_dev[-1])\n",
    "\n",
    "        gradient_descent_batch(batch_data, batch_labels,\n",
    "            learning_rate, params, backward_prop_func)\n",
    "\n",
    "    return params, cost_dev, accuracy_dev"
   ],
   "id": "6b5e545fb980d250",
   "outputs": [],
   "execution_count": 346
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T12:48:57.802303Z",
     "start_time": "2025-09-03T12:48:57.796540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def nn_test(data, labels, params):\n",
    "    output, cost = forward_prop(data, labels, params)\n",
    "    accuracy = compute_accuracy(output, labels)\n",
    "    return accuracy"
   ],
   "id": "bdfceec213c624c6",
   "outputs": [],
   "execution_count": 347
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T12:48:57.858540Z",
     "start_time": "2025-09-03T12:48:57.851399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_accuracy(output, labels):\n",
    "    correct_output = np.argmax(output,axis=1)\n",
    "    correct_labels = np.argmax(labels,axis=1)\n",
    "\n",
    "    is_correct = [a == b for a,b in zip(correct_output, correct_labels)]\n",
    "\n",
    "    accuracy = sum(is_correct) * 1. / labels.shape[0]\n",
    "    return accuracy"
   ],
   "id": "b26f357572febc51",
   "outputs": [],
   "execution_count": 348
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T12:48:57.912965Z",
     "start_time": "2025-09-03T12:48:57.907499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def one_hot_labels(labels):\n",
    "    one_hot_labels = np.zeros((labels.size, 10))\n",
    "    one_hot_labels[np.arange(labels.size),labels.astype(int)] = 1\n",
    "    return one_hot_labels"
   ],
   "id": "66661881ff819c63",
   "outputs": [],
   "execution_count": 349
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T12:48:57.969140Z",
     "start_time": "2025-09-03T12:48:57.963081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_data(images_file, labels_file):\n",
    "    x = np.loadtxt(images_file, delimiter=',')\n",
    "    y = np.loadtxt(labels_file, delimiter=',')\n",
    "\n",
    "    x = np.reshape(x, (x.shape[0], 1, 28, 28))\n",
    "\n",
    "    return x, y"
   ],
   "id": "6dfbef4f4cb20814",
   "outputs": [],
   "execution_count": 350
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T12:48:58.027470Z",
     "start_time": "2025-09-03T12:48:58.017674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_train(all_data, all_labels, backward_prop_func):\n",
    "    params, cost_dev, accuracy_dev = nn_train(\n",
    "        all_data['train'], all_labels['train'],\n",
    "        all_data['dev'], all_labels['dev'],\n",
    "        get_initial_params, forward_prop, backward_prop_func,\n",
    "        learning_rate=1e-2, batch_size=16, num_batches=400\n",
    "    )\n",
    "\n",
    "    t = np.arange(400 // 100)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "\n",
    "    ax1.plot(t, cost_dev, 'b')\n",
    "    ax1.set_xlabel('time')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.set_title('Training curve')\n",
    "\n",
    "    ax2.plot(t, accuracy_dev, 'b')\n",
    "    ax2.set_xlabel('time')\n",
    "    ax2.set_ylabel('accuracy')\n",
    "\n",
    "    fig.savefig('output/train.png')"
   ],
   "id": "fa758699cdded78d",
   "outputs": [],
   "execution_count": 351
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training Model",
   "id": "39fb3b09a301e4cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T12:48:58.087202Z",
     "start_time": "2025-09-03T12:48:58.076539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    np.random.seed(100)\n",
    "    train_data, train_labels = read_data('/home/anhnt02/Desktop/data_1/images_train.csv', '/home/anhnt02/Desktop/data_1/labels_train.csv')\n",
    "    train_labels = one_hot_labels(train_labels)\n",
    "    p = np.random.permutation(60000)\n",
    "    train_data = train_data[p,:]\n",
    "    train_labels = train_labels[p,:]\n",
    "\n",
    "    dev_data = train_data[0:400,:]\n",
    "    dev_labels = train_labels[0:400,:]\n",
    "    train_data = train_data[400:,:]\n",
    "    train_labels = train_labels[400:,:]\n",
    "\n",
    "    mean = np.mean(train_data)\n",
    "    std = np.std(train_data)\n",
    "    train_data = (train_data - mean) / std\n",
    "    dev_data = (dev_data - mean) / std\n",
    "\n",
    "    all_data = {\n",
    "        'train': train_data,\n",
    "        'dev': dev_data,\n",
    "    }\n",
    "\n",
    "    all_labels = {\n",
    "        'train': train_labels,\n",
    "        'dev': dev_labels,\n",
    "    }\n",
    "\n",
    "    run_train(all_data, all_labels, backward_prop)"
   ],
   "id": "a7bdf0c15e95a315",
   "outputs": [],
   "execution_count": 352
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T12:57:19.006186Z",
     "start_time": "2025-09-03T12:48:58.137413Z"
    }
   },
   "cell_type": "code",
   "source": "main()",
   "id": "b435cc7284f462d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing 0 / 400\n",
      "Cost and accuracy 2.721417647426753 0.0725\n",
      "Currently processing 1 / 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anhnt02/miniconda3/envs/cs229/lib/python3.6/site-packages/ipykernel_launcher.py:30: RuntimeWarning: overflow encountered in double_scalars\n",
      "/home/anhnt02/miniconda3/envs/cs229/lib/python3.6/site-packages/ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/anhnt02/miniconda3/envs/cs229/lib/python3.6/site-packages/ipykernel_launcher.py:62: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/anhnt02/miniconda3/envs/cs229/lib/python3.6/site-packages/ipykernel_launcher.py:70: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/anhnt02/miniconda3/envs/cs229/lib/python3.6/site-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/anhnt02/miniconda3/envs/cs229/lib/python3.6/site-packages/ipykernel_launcher.py:30: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing 2 / 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anhnt02/miniconda3/envs/cs229/lib/python3.6/site-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in less_equal\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/anhnt02/miniconda3/envs/cs229/lib/python3.6/site-packages/ipykernel_launcher.py:53: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing 3 / 400\n",
      "Currently processing 4 / 400\n",
      "Currently processing 5 / 400\n",
      "Currently processing 6 / 400\n",
      "Currently processing 7 / 400\n",
      "Currently processing 8 / 400\n",
      "Currently processing 9 / 400\n",
      "Currently processing 10 / 400\n",
      "Currently processing 11 / 400\n",
      "Currently processing 12 / 400\n",
      "Currently processing 13 / 400\n",
      "Currently processing 14 / 400\n",
      "Currently processing 15 / 400\n",
      "Currently processing 16 / 400\n",
      "Currently processing 17 / 400\n",
      "Currently processing 18 / 400\n",
      "Currently processing 19 / 400\n",
      "Currently processing 20 / 400\n",
      "Currently processing 21 / 400\n",
      "Currently processing 22 / 400\n",
      "Currently processing 23 / 400\n",
      "Currently processing 24 / 400\n",
      "Currently processing 25 / 400\n",
      "Currently processing 26 / 400\n",
      "Currently processing 27 / 400\n",
      "Currently processing 28 / 400\n",
      "Currently processing 29 / 400\n",
      "Currently processing 30 / 400\n",
      "Currently processing 31 / 400\n",
      "Currently processing 32 / 400\n",
      "Currently processing 33 / 400\n",
      "Currently processing 34 / 400\n",
      "Currently processing 35 / 400\n",
      "Currently processing 36 / 400\n",
      "Currently processing 37 / 400\n",
      "Currently processing 38 / 400\n",
      "Currently processing 39 / 400\n",
      "Currently processing 40 / 400\n",
      "Currently processing 41 / 400\n",
      "Currently processing 42 / 400\n",
      "Currently processing 43 / 400\n",
      "Currently processing 44 / 400\n",
      "Currently processing 45 / 400\n",
      "Currently processing 46 / 400\n",
      "Currently processing 47 / 400\n",
      "Currently processing 48 / 400\n",
      "Currently processing 49 / 400\n",
      "Currently processing 50 / 400\n",
      "Currently processing 51 / 400\n",
      "Currently processing 52 / 400\n",
      "Currently processing 53 / 400\n",
      "Currently processing 54 / 400\n",
      "Currently processing 55 / 400\n",
      "Currently processing 56 / 400\n",
      "Currently processing 57 / 400\n",
      "Currently processing 58 / 400\n",
      "Currently processing 59 / 400\n",
      "Currently processing 60 / 400\n",
      "Currently processing 61 / 400\n",
      "Currently processing 62 / 400\n",
      "Currently processing 63 / 400\n",
      "Currently processing 64 / 400\n",
      "Currently processing 65 / 400\n",
      "Currently processing 66 / 400\n",
      "Currently processing 67 / 400\n",
      "Currently processing 68 / 400\n",
      "Currently processing 69 / 400\n",
      "Currently processing 70 / 400\n",
      "Currently processing 71 / 400\n",
      "Currently processing 72 / 400\n",
      "Currently processing 73 / 400\n",
      "Currently processing 74 / 400\n",
      "Currently processing 75 / 400\n",
      "Currently processing 76 / 400\n",
      "Currently processing 77 / 400\n",
      "Currently processing 78 / 400\n",
      "Currently processing 79 / 400\n",
      "Currently processing 80 / 400\n",
      "Currently processing 81 / 400\n",
      "Currently processing 82 / 400\n",
      "Currently processing 83 / 400\n",
      "Currently processing 84 / 400\n",
      "Currently processing 85 / 400\n",
      "Currently processing 86 / 400\n",
      "Currently processing 87 / 400\n",
      "Currently processing 88 / 400\n",
      "Currently processing 89 / 400\n",
      "Currently processing 90 / 400\n",
      "Currently processing 91 / 400\n",
      "Currently processing 92 / 400\n",
      "Currently processing 93 / 400\n",
      "Currently processing 94 / 400\n",
      "Currently processing 95 / 400\n",
      "Currently processing 96 / 400\n",
      "Currently processing 97 / 400\n",
      "Currently processing 98 / 400\n",
      "Currently processing 99 / 400\n",
      "Currently processing 100 / 400\n",
      "Cost and accuracy nan 0.1175\n",
      "Currently processing 101 / 400\n",
      "Currently processing 102 / 400\n",
      "Currently processing 103 / 400\n",
      "Currently processing 104 / 400\n",
      "Currently processing 105 / 400\n",
      "Currently processing 106 / 400\n",
      "Currently processing 107 / 400\n",
      "Currently processing 108 / 400\n",
      "Currently processing 109 / 400\n",
      "Currently processing 110 / 400\n",
      "Currently processing 111 / 400\n",
      "Currently processing 112 / 400\n",
      "Currently processing 113 / 400\n",
      "Currently processing 114 / 400\n",
      "Currently processing 115 / 400\n",
      "Currently processing 116 / 400\n",
      "Currently processing 117 / 400\n",
      "Currently processing 118 / 400\n",
      "Currently processing 119 / 400\n",
      "Currently processing 120 / 400\n",
      "Currently processing 121 / 400\n",
      "Currently processing 122 / 400\n",
      "Currently processing 123 / 400\n",
      "Currently processing 124 / 400\n",
      "Currently processing 125 / 400\n",
      "Currently processing 126 / 400\n",
      "Currently processing 127 / 400\n",
      "Currently processing 128 / 400\n",
      "Currently processing 129 / 400\n",
      "Currently processing 130 / 400\n",
      "Currently processing 131 / 400\n",
      "Currently processing 132 / 400\n",
      "Currently processing 133 / 400\n",
      "Currently processing 134 / 400\n",
      "Currently processing 135 / 400\n",
      "Currently processing 136 / 400\n",
      "Currently processing 137 / 400\n",
      "Currently processing 138 / 400\n",
      "Currently processing 139 / 400\n",
      "Currently processing 140 / 400\n",
      "Currently processing 141 / 400\n",
      "Currently processing 142 / 400\n",
      "Currently processing 143 / 400\n",
      "Currently processing 144 / 400\n",
      "Currently processing 145 / 400\n",
      "Currently processing 146 / 400\n",
      "Currently processing 147 / 400\n",
      "Currently processing 148 / 400\n",
      "Currently processing 149 / 400\n",
      "Currently processing 150 / 400\n",
      "Currently processing 151 / 400\n",
      "Currently processing 152 / 400\n",
      "Currently processing 153 / 400\n",
      "Currently processing 154 / 400\n",
      "Currently processing 155 / 400\n",
      "Currently processing 156 / 400\n",
      "Currently processing 157 / 400\n",
      "Currently processing 158 / 400\n",
      "Currently processing 159 / 400\n",
      "Currently processing 160 / 400\n",
      "Currently processing 161 / 400\n",
      "Currently processing 162 / 400\n",
      "Currently processing 163 / 400\n",
      "Currently processing 164 / 400\n",
      "Currently processing 165 / 400\n",
      "Currently processing 166 / 400\n",
      "Currently processing 167 / 400\n",
      "Currently processing 168 / 400\n",
      "Currently processing 169 / 400\n",
      "Currently processing 170 / 400\n",
      "Currently processing 171 / 400\n",
      "Currently processing 172 / 400\n",
      "Currently processing 173 / 400\n",
      "Currently processing 174 / 400\n",
      "Currently processing 175 / 400\n",
      "Currently processing 176 / 400\n",
      "Currently processing 177 / 400\n",
      "Currently processing 178 / 400\n",
      "Currently processing 179 / 400\n",
      "Currently processing 180 / 400\n",
      "Currently processing 181 / 400\n",
      "Currently processing 182 / 400\n",
      "Currently processing 183 / 400\n",
      "Currently processing 184 / 400\n",
      "Currently processing 185 / 400\n",
      "Currently processing 186 / 400\n",
      "Currently processing 187 / 400\n",
      "Currently processing 188 / 400\n",
      "Currently processing 189 / 400\n",
      "Currently processing 190 / 400\n",
      "Currently processing 191 / 400\n",
      "Currently processing 192 / 400\n",
      "Currently processing 193 / 400\n",
      "Currently processing 194 / 400\n",
      "Currently processing 195 / 400\n",
      "Currently processing 196 / 400\n",
      "Currently processing 197 / 400\n",
      "Currently processing 198 / 400\n",
      "Currently processing 199 / 400\n",
      "Currently processing 200 / 400\n",
      "Cost and accuracy nan 0.1175\n",
      "Currently processing 201 / 400\n",
      "Currently processing 202 / 400\n",
      "Currently processing 203 / 400\n",
      "Currently processing 204 / 400\n",
      "Currently processing 205 / 400\n",
      "Currently processing 206 / 400\n",
      "Currently processing 207 / 400\n",
      "Currently processing 208 / 400\n",
      "Currently processing 209 / 400\n",
      "Currently processing 210 / 400\n",
      "Currently processing 211 / 400\n",
      "Currently processing 212 / 400\n",
      "Currently processing 213 / 400\n",
      "Currently processing 214 / 400\n",
      "Currently processing 215 / 400\n",
      "Currently processing 216 / 400\n",
      "Currently processing 217 / 400\n",
      "Currently processing 218 / 400\n",
      "Currently processing 219 / 400\n",
      "Currently processing 220 / 400\n",
      "Currently processing 221 / 400\n",
      "Currently processing 222 / 400\n",
      "Currently processing 223 / 400\n",
      "Currently processing 224 / 400\n",
      "Currently processing 225 / 400\n",
      "Currently processing 226 / 400\n",
      "Currently processing 227 / 400\n",
      "Currently processing 228 / 400\n",
      "Currently processing 229 / 400\n",
      "Currently processing 230 / 400\n",
      "Currently processing 231 / 400\n",
      "Currently processing 232 / 400\n",
      "Currently processing 233 / 400\n",
      "Currently processing 234 / 400\n",
      "Currently processing 235 / 400\n",
      "Currently processing 236 / 400\n",
      "Currently processing 237 / 400\n",
      "Currently processing 238 / 400\n",
      "Currently processing 239 / 400\n",
      "Currently processing 240 / 400\n",
      "Currently processing 241 / 400\n",
      "Currently processing 242 / 400\n",
      "Currently processing 243 / 400\n",
      "Currently processing 244 / 400\n",
      "Currently processing 245 / 400\n",
      "Currently processing 246 / 400\n",
      "Currently processing 247 / 400\n",
      "Currently processing 248 / 400\n",
      "Currently processing 249 / 400\n",
      "Currently processing 250 / 400\n",
      "Currently processing 251 / 400\n",
      "Currently processing 252 / 400\n",
      "Currently processing 253 / 400\n",
      "Currently processing 254 / 400\n",
      "Currently processing 255 / 400\n",
      "Currently processing 256 / 400\n",
      "Currently processing 257 / 400\n",
      "Currently processing 258 / 400\n",
      "Currently processing 259 / 400\n",
      "Currently processing 260 / 400\n",
      "Currently processing 261 / 400\n",
      "Currently processing 262 / 400\n",
      "Currently processing 263 / 400\n",
      "Currently processing 264 / 400\n",
      "Currently processing 265 / 400\n",
      "Currently processing 266 / 400\n",
      "Currently processing 267 / 400\n",
      "Currently processing 268 / 400\n",
      "Currently processing 269 / 400\n",
      "Currently processing 270 / 400\n",
      "Currently processing 271 / 400\n",
      "Currently processing 272 / 400\n",
      "Currently processing 273 / 400\n",
      "Currently processing 274 / 400\n",
      "Currently processing 275 / 400\n",
      "Currently processing 276 / 400\n",
      "Currently processing 277 / 400\n",
      "Currently processing 278 / 400\n",
      "Currently processing 279 / 400\n",
      "Currently processing 280 / 400\n",
      "Currently processing 281 / 400\n",
      "Currently processing 282 / 400\n",
      "Currently processing 283 / 400\n",
      "Currently processing 284 / 400\n",
      "Currently processing 285 / 400\n",
      "Currently processing 286 / 400\n",
      "Currently processing 287 / 400\n",
      "Currently processing 288 / 400\n",
      "Currently processing 289 / 400\n",
      "Currently processing 290 / 400\n",
      "Currently processing 291 / 400\n",
      "Currently processing 292 / 400\n",
      "Currently processing 293 / 400\n",
      "Currently processing 294 / 400\n",
      "Currently processing 295 / 400\n",
      "Currently processing 296 / 400\n",
      "Currently processing 297 / 400\n",
      "Currently processing 298 / 400\n",
      "Currently processing 299 / 400\n",
      "Currently processing 300 / 400\n",
      "Cost and accuracy nan 0.1175\n",
      "Currently processing 301 / 400\n",
      "Currently processing 302 / 400\n",
      "Currently processing 303 / 400\n",
      "Currently processing 304 / 400\n",
      "Currently processing 305 / 400\n",
      "Currently processing 306 / 400\n",
      "Currently processing 307 / 400\n",
      "Currently processing 308 / 400\n",
      "Currently processing 309 / 400\n",
      "Currently processing 310 / 400\n",
      "Currently processing 311 / 400\n",
      "Currently processing 312 / 400\n",
      "Currently processing 313 / 400\n",
      "Currently processing 314 / 400\n",
      "Currently processing 315 / 400\n",
      "Currently processing 316 / 400\n",
      "Currently processing 317 / 400\n",
      "Currently processing 318 / 400\n",
      "Currently processing 319 / 400\n",
      "Currently processing 320 / 400\n",
      "Currently processing 321 / 400\n",
      "Currently processing 322 / 400\n",
      "Currently processing 323 / 400\n",
      "Currently processing 324 / 400\n",
      "Currently processing 325 / 400\n",
      "Currently processing 326 / 400\n",
      "Currently processing 327 / 400\n",
      "Currently processing 328 / 400\n",
      "Currently processing 329 / 400\n",
      "Currently processing 330 / 400\n",
      "Currently processing 331 / 400\n",
      "Currently processing 332 / 400\n",
      "Currently processing 333 / 400\n",
      "Currently processing 334 / 400\n",
      "Currently processing 335 / 400\n",
      "Currently processing 336 / 400\n",
      "Currently processing 337 / 400\n",
      "Currently processing 338 / 400\n",
      "Currently processing 339 / 400\n",
      "Currently processing 340 / 400\n",
      "Currently processing 341 / 400\n",
      "Currently processing 342 / 400\n",
      "Currently processing 343 / 400\n",
      "Currently processing 344 / 400\n",
      "Currently processing 345 / 400\n",
      "Currently processing 346 / 400\n",
      "Currently processing 347 / 400\n",
      "Currently processing 348 / 400\n",
      "Currently processing 349 / 400\n",
      "Currently processing 350 / 400\n",
      "Currently processing 351 / 400\n",
      "Currently processing 352 / 400\n",
      "Currently processing 353 / 400\n",
      "Currently processing 354 / 400\n",
      "Currently processing 355 / 400\n",
      "Currently processing 356 / 400\n",
      "Currently processing 357 / 400\n",
      "Currently processing 358 / 400\n",
      "Currently processing 359 / 400\n",
      "Currently processing 360 / 400\n",
      "Currently processing 361 / 400\n",
      "Currently processing 362 / 400\n",
      "Currently processing 363 / 400\n",
      "Currently processing 364 / 400\n",
      "Currently processing 365 / 400\n",
      "Currently processing 366 / 400\n",
      "Currently processing 367 / 400\n",
      "Currently processing 368 / 400\n",
      "Currently processing 369 / 400\n",
      "Currently processing 370 / 400\n",
      "Currently processing 371 / 400\n",
      "Currently processing 372 / 400\n",
      "Currently processing 373 / 400\n",
      "Currently processing 374 / 400\n",
      "Currently processing 375 / 400\n",
      "Currently processing 376 / 400\n",
      "Currently processing 377 / 400\n",
      "Currently processing 378 / 400\n",
      "Currently processing 379 / 400\n",
      "Currently processing 380 / 400\n",
      "Currently processing 381 / 400\n",
      "Currently processing 382 / 400\n",
      "Currently processing 383 / 400\n",
      "Currently processing 384 / 400\n",
      "Currently processing 385 / 400\n",
      "Currently processing 386 / 400\n",
      "Currently processing 387 / 400\n",
      "Currently processing 388 / 400\n",
      "Currently processing 389 / 400\n",
      "Currently processing 390 / 400\n",
      "Currently processing 391 / 400\n",
      "Currently processing 392 / 400\n",
      "Currently processing 393 / 400\n",
      "Currently processing 394 / 400\n",
      "Currently processing 395 / 400\n",
      "Currently processing 396 / 400\n",
      "Currently processing 397 / 400\n",
      "Currently processing 398 / 400\n",
      "Currently processing 399 / 400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 353
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
