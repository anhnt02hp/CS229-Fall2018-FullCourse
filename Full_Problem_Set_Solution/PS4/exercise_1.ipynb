{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "# PS4-1: Neural Networks: MNIST Image Classification",
   "id": "5ef2d0779a09a5b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Library",
   "id": "96393687726f3b9c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T13:59:43.365312Z",
     "start_time": "2025-09-01T13:59:42.536861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ],
   "id": "af4ac1e784f2cb92",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Global Parameters",
   "id": "30c2bd9c4dc33a34"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T14:00:05.438914Z",
     "start_time": "2025-09-01T14:00:05.432756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_POOL_SIZE = 5\n",
    "CONVOLUTION_SIZE = 4\n",
    "CONVOLUTION_FILTERS = 2"
   ],
   "id": "573b1d5abad9eb94",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training NN Algorithm",
   "id": "3e0bc68626bbc8b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training Algorithm",
   "id": "f73616598932fb8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def forward_softmax(x):\n",
    "    \"\"\"\n",
    "    Compute softmax function for a single example.\n",
    "    The shape of the input is of size # num classes.\n",
    "\n",
    "    Important Note: You must be careful to avoid overflow for this function. Functions\n",
    "    like softmax have a tendency to overflow when very large numbers like e^10000 are computed.\n",
    "    You will know that your function is overflow resistent when it can handle input like:\n",
    "    np.array([[10000, 10010, 10]]) without issues.\n",
    "\n",
    "        x: A 1d numpy float array of shape number_of_classes\n",
    "\n",
    "    Returns:\n",
    "        A 1d numpy float array containing the softmax results of shape  number_of_classes\n",
    "    \"\"\"\n",
    "    x = x - np.max(x,axis=0)\n",
    "    exp = np.exp(x)\n",
    "    s = exp / np.sum(exp,axis=0)\n",
    "    return s"
   ],
   "id": "ec594d5c6f7ddd9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def backward_softmax(x, grad_outputs):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with respect to x.\n",
    "\n",
    "    grad_outputs is the gradient of the loss with respect to the outputs of the softmax.\n",
    "\n",
    "    Args:\n",
    "        x: A 1d numpy float array of shape number_of_classes\n",
    "        grad_outputs: A 1d numpy float array of shape number_of_classes\n",
    "\n",
    "    Returns:\n",
    "        A 1d numpy float array of the same shape as x with the derivative of the loss with respect to x\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    \"\"\"\n",
    "    We need to compute derivative of CE loss w.r.t x\n",
    "    Note: x = logits (input of softmax) => x is 1D, y_hat = softmax(x)\n",
    "    Formula: dCE/dx = dCE/dy_hat * dy_hat/dx (Chain Rule), where:\n",
    "        dCE/dy_hat = grad_outputs\n",
    "        dy_hat/dx = Derivative of y_hat w.r.t x\n",
    "    \"\"\"\n",
    "    # 1. Get x size = number_of_classes\n",
    "    K = len(x)\n",
    "\n",
    "    # 2. Calculate Softmax\n",
    "    y_hat = forward_softmax(x)\n",
    "\n",
    "    # 3. Update dCE/dx\n",
    "    d_CE_x = np.zeros(K)\n",
    "\n",
    "    for j in range(K):\n",
    "        for k in range(K):\n",
    "            if j == k:\n",
    "                d_CE_x[j] += grad_outputs[k] * y_hat[k] * (1 - y_hat[k])\n",
    "            else:\n",
    "                d_CE_x[j] += grad_outputs[k] * (- y_hat[k] * y_hat[j])\n",
    "\n",
    "    return d_CE_x\n",
    "    # *** END CODE HERE ***"
   ],
   "id": "ce0ddf35c0a83b01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def forward_relu(x):\n",
    "    \"\"\"\n",
    "    Compute the relu function for the input x.\n",
    "\n",
    "    Args:\n",
    "        x: A numpy float array\n",
    "\n",
    "    Returns:\n",
    "        A numpy float array containing the relu results\n",
    "    \"\"\"\n",
    "    x[x<=0] = 0\n",
    "\n",
    "    return x"
   ],
   "id": "fddfbb65a14a33f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def backward_relu(x, grad_outputs):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with respect to x\n",
    "\n",
    "    Args:\n",
    "        x: A numpy array of arbitrary shape containing the input.\n",
    "        grad_outputs: A numpy array of the same shape of x containing the gradient of the loss with respect\n",
    "            to the output of relu\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of the same shape as x containing the gradients with respect to x.\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    \"\"\"\n",
    "    We need to compute derivative of \"Cross-Entropy Loss is calculated after Softmax\" loss w.r.t x\n",
    "    Note: x = MaxPooling output (input of ReLU) => x is 3D image (Batch Size, Channels, Heights, Widths)\n",
    "    Formula: dL/dx = dL/reLU * dreLU/dx (Chain Rule), where:\n",
    "        dL/dreLU = grad_outputs\n",
    "        dreLU/dx  = Derivative of reLU w.r.t x\n",
    "    \"\"\"\n",
    "    # 1. Get x shape\n",
    "    B, C, H, W = x.shape\n",
    "\n",
    "    # 2. Update dL/dreLU\n",
    "    grad_x = np.zeros(x.shape)\n",
    "    for b in range(B):\n",
    "        for c in range(C):\n",
    "            for h in range(H):\n",
    "                for w in range(W):\n",
    "                    if x[b, c, h, w] > 0:\n",
    "                        grad_x[b, c, h, w] = 1 * grad_outputs[b, c, h, w]\n",
    "                    else:\n",
    "                        grad_x[b, c, h, w] = 0\n",
    "\n",
    "    return grad_x\n",
    "    # *** END CODE HERE ***"
   ],
   "id": "4da374176cb978bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_initial_params():\n",
    "    \"\"\"\n",
    "    Compute the initial parameters for the neural network.\n",
    "\n",
    "    This function should return a dictionary mapping parameter names to numpy arrays containing\n",
    "    the initial values for those parameters.\n",
    "\n",
    "    There should be four parameters for this model:\n",
    "    W1 is the weight matrix for the convolutional layer\n",
    "    b1 is the bias vector for the convolutional layer\n",
    "    W2 is the weight matrix for the output layers\n",
    "    b2 is the bias vector for the output layer\n",
    "\n",
    "    Weight matrices should be initialized with values drawn from a random normal distribution.\n",
    "    The mean of that distribution should be 0.\n",
    "    The variance of that distribution should be 1/sqrt(n) where n is the number of neurons that\n",
    "    feed into an output for that layer.\n",
    "\n",
    "    Bias vectors should be initialized with zero.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        A dict mapping parameter names to numpy arrays\n",
    "    \"\"\"\n",
    "\n",
    "    size_after_convolution = 28 - CONVOLUTION_SIZE + 1\n",
    "    size_after_max_pooling = size_after_convolution // MAX_POOL_SIZE\n",
    "\n",
    "    num_hidden = size_after_max_pooling * size_after_max_pooling * CONVOLUTION_FILTERS\n",
    "\n",
    "    return {\n",
    "        'W1': np.random.normal(size = (CONVOLUTION_FILTERS, 1, CONVOLUTION_SIZE, CONVOLUTION_SIZE), scale=1/ math.sqrt(CONVOLUTION_SIZE * CONVOLUTION_SIZE)),\n",
    "        'b1': np.zeros(CONVOLUTION_FILTERS),\n",
    "        'W2': np.random.normal(size = (num_hidden, 10), scale = 1/ math.sqrt(num_hidden)),\n",
    "        'b2': np.zeros(10)\n",
    "    }"
   ],
   "id": "a2eb67b0070ee17d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def forward_convolution(conv_W, conv_b, data):\n",
    "    \"\"\"\n",
    "    Compute the output from a convolutional layer given the weights and data.\n",
    "\n",
    "    conv_W is of the shape (# output channels, # input channels, convolution width, convolution height )\n",
    "    conv_b is of the shape (# output channels)\n",
    "\n",
    "    data is of the shape (# input channels, width, height)\n",
    "\n",
    "    The output should be the result of a convolution and should be of the size:\n",
    "        (# output channels, width - convolution width + 1, height -  convolution height + 1)\n",
    "\n",
    "    Returns:\n",
    "        The output of the convolution as a numpy array\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    oc = output_channels\n",
    "    ic = input_channels\n",
    "    x = starting coordinates in width.\n",
    "    y = starting coordinates in height.\n",
    "    di = conv_W_offset_width\n",
    "    dj = conv_W_offset_height\n",
    "    data = input data\n",
    "\n",
    "    Formula:\n",
    "    output[oc, x, y] = conv_b[oc] + sum_di,sum_dj,sum_ic (data[ic, x + di, y + dj] * conv_W[oc, ic, di, dj])\n",
    "    \"\"\"\n",
    "    conv_channels, _, conv_width, conv_height = conv_W.shape\n",
    "\n",
    "    input_channels, input_width, input_height = data.shape\n",
    "\n",
    "    output = np.zeros((conv_channels, input_width - conv_width + 1, input_height - conv_height + 1))\n",
    "\n",
    "    for x in range(input_width - conv_width + 1):\n",
    "        for y in range(input_height - conv_height + 1):\n",
    "            for output_channel in range(conv_channels):\n",
    "                output[output_channel, x, y] = np.sum(\n",
    "                    np.multiply(data[:, x:(x + conv_width), y:(y + conv_height)], conv_W[output_channel, :, :, :])) + conv_b[output_channel]\n",
    "\n",
    "    return output"
   ],
   "id": "35700cc30c212df1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def backward_convolution(conv_W, conv_b, data, output_grad):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with respect to the parameters of the convolution.\n",
    "\n",
    "    See forward_convolution for the sizes of the arguments.\n",
    "    output_grad is the gradient of the loss with respect to the output of the convolution.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing 3 gradients.\n",
    "        The first element is the gradient of the loss with respect to the convolution weights\n",
    "        The second element is the gradient of the loss with respect to the convolution bias\n",
    "        The third element is the gradient of the loss with respect to the input data\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    \"\"\"\n",
    "    We need to compute derivative of \"Loss Function\" loss w.r.t conv_W, conv_b, data in tuple\n",
    "\n",
    "    Parameters:\n",
    "    oc = output_channels\n",
    "    ic = input_channels\n",
    "    x = starting coordinates in width (data)\n",
    "    y = starting coordinates in height (data)\n",
    "    di = conv_W_offset_width\n",
    "    dj = conv_W_offset_height\n",
    "    data = input data\n",
    "\n",
    "    Formula:\n",
    "    dL/doutput = dL/doutput[oc, x, y] === output_grad\n",
    "\n",
    "    doutput/dconv_W = doutput[oc, x, y]/dconv_W[oc, ic, di, dj] === data[ic, x + di, y + dj]\n",
    "\n",
    "    doutput/dconv_b = doutput[oc, x, y]/dconv_b[oc] === 1\n",
    "\n",
    "    doutput/ddata = doutput[oc, x, y]/ddata[ic, x+di, y+dj] === conv_W[oc, ic, di, dj]\n",
    "\n",
    "    So, we have full Formula:\n",
    "    1. dL/dconv_W = dL/doutput * doutput/dconv_W === sum_x,sum_y (output_grad[oc, x, y] * data[ic, x + di, y + dj])\n",
    "\n",
    "    2. dL/dconv_b = dL/doutput * doutput/dconv_b === sum_x, sum_y (output_grad[oc, x, y] * 1)\n",
    "\n",
    "    3. dL/ddata = dL/doutput * doutput/ddata === sum_oc, sum_di, sum_dj (output_grad[oc, x-di, y-dj] * conv_W[oc, ic, di, dj])\n",
    "    \"\"\"\n",
    "    # 1. Get Parameters\n",
    "    conv_channels, _, conv_width, conv_height = conv_W.shape\n",
    "    input_channels, input_width, input_height = data.shape\n",
    "    _, output_width, output_height = output_grad.shape\n",
    "\n",
    "    # 2. Initialize Gradient\n",
    "    dW = np.zeros(conv_W.shape)\n",
    "    db = np.zeros(conv_b.shape)\n",
    "    ddata = np.zeros(data.shape)\n",
    "\n",
    "    # 3. dL/dconv_W\n",
    "    for oc in range(conv_channels):\n",
    "        for ic in range(input_channels):\n",
    "            for di in range(conv_width):\n",
    "                for dj in range(conv_height):\n",
    "                    temp_grad = 0\n",
    "                    for x in range(output_width):\n",
    "                        for y in range(output_height):\n",
    "                            temp_grad += output_grad[oc, x, y] * data[ic, x + di, y + dj]\n",
    "                    dW[oc, ic, di, dj] = temp_grad\n",
    "\n",
    "    # 4. dL/dconv_b\n",
    "    for oc in range(conv_channels):\n",
    "        temp_grad = 0\n",
    "        for x in range(output_width):\n",
    "            for y in range(output_height):\n",
    "                temp_grad += output_grad[oc, x, y] * 1\n",
    "        db[oc] = temp_grad\n",
    "\n",
    "    # 5. dL/ddata\n",
    "    for oc in range(conv_channels):\n",
    "        for ic in range(input_channels):\n",
    "            for x in range(input_width):\n",
    "                for y in range(input_height):\n",
    "                    temp_grad = 0\n",
    "                    for di in range(conv_width):\n",
    "                        for dj in range(conv_height):\n",
    "                            x_out = x - di\n",
    "                            y_out = y - dj\n",
    "                            if 0 <= x_out < output_width and 0 <= y_out < output_height:\n",
    "                                temp_grad += output_grad[oc, x_out, y_out] * conv_W[oc, ic, di, dj]\n",
    "                    ddata[ic, x, y] = temp_grad\n",
    "\n",
    "    return dW, db, ddata\n",
    "    # *** END CODE HERE ***"
   ],
   "id": "c15417e8b8c81c93"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def forward_max_pool(data, pool_width, pool_height):\n",
    "    \"\"\"\n",
    "    Compute the output from a max pooling layer given the data and pool dimensions.\n",
    "\n",
    "    The stride length should be equal to the pool size\n",
    "\n",
    "    data is of the shape (# channels, width, height)\n",
    "\n",
    "    The output should be the result of the max pooling layer and should be of size:\n",
    "        (# channels, width // pool_width, height // pool_height)\n",
    "\n",
    "    Returns:\n",
    "        The result of the max pooling layer\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    oc = output_channels\n",
    "    ic = input_channels\n",
    "    x = starting coordinates in width (data)\n",
    "    y = starting coordinates in height (data)\n",
    "    di = conv_W_offset_width\n",
    "    dj = conv_W_offset_height\n",
    "    data = input data\n",
    "\n",
    "    Formula:\n",
    "    output[oc, x, y] = max_di,dj (data[ic, x * pool_width + di, y * pool_height + dj])\n",
    "    \"\"\"\n",
    "    input_channels, input_width, input_height = data.shape\n",
    "\n",
    "    output = np.zeros((input_channels, input_width // pool_width, input_height // pool_height))\n",
    "\n",
    "    for x in range(0, input_width, pool_width):\n",
    "        for y in range(0, input_height, pool_height):\n",
    "\n",
    "            output[:, x // pool_width, y // pool_height] = np.amax(data[:, x:(x + pool_width), y:(y + pool_height)], axis=(1, 2))\n",
    "\n",
    "    return output"
   ],
   "id": "d913c8ec5222e122"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def backward_max_pool(data, pool_width, pool_height, output_grad):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with respect to the data in the max pooling layer.\n",
    "\n",
    "    data is of the shape (# channels, width, height)\n",
    "    output_grad is of shape (# channels, width // pool_width, height // pool_height)\n",
    "\n",
    "    output_grad is the gradient of the loss with respect to the output of the backward max\n",
    "    pool layer.\n",
    "\n",
    "    Returns:\n",
    "        The gradient of the loss with respect to the data (of same shape as data)\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    \"\"\"\n",
    "    We need to compute derivative of \"Loss Function\" loss w.r.t data\n",
    "\n",
    "    Parameters:\n",
    "    oc = output_channels\n",
    "    ic = input_channels\n",
    "    x = starting coordinates in width (data)\n",
    "    y = starting coordinates in height (data)\n",
    "    di = step of pool_width\n",
    "    dj = step of pool_height\n",
    "    data = input data\n",
    "    x_out = x/pool_width\n",
    "    y_out = y/pool_height\n",
    "\n",
    "    Formula:\n",
    "    dL/doutput = dL/doutput[oc, x, y] === output_grad[oc, x_out, y_out]\n",
    "\n",
    "    doutput/ddata = doutput[oc, x, y]/ddata[ic, x * pool_width + di, y * pool_height + dj] === 1 (if (i,j) is max in (x_out, y_out)) else === 0\n",
    "\n",
    "    So, we have ful Formula:\n",
    "    dL/ddata = dL/doutput * doutput/ddata === output_grad[oc, x_out, y_out] * 1 (if (i,j) is max in (x_out, y_out)) or === 0\n",
    "    \"\"\"\n",
    "    # 1. Get Parameters\n",
    "    input_channels, input_width, input_height = data.shape\n",
    "    x_out, y_out = input_width // pool_width, input_height // pool_height\n",
    "    _, output_width, output_height = output_grad.shape\n",
    "\n",
    "    # 2. Initialize Parameters\n",
    "    ddata = np.zeros(data.shape)\n",
    "\n",
    "    # 3. dL/ddata\n",
    "    for ic in range(input_channels):\n",
    "        for x_out in range(output_width):\n",
    "            for y_out in range(output_height):\n",
    "                max_i, max_j = 0, 0\n",
    "                max_value = -1e9\n",
    "                for di in range(pool_width):\n",
    "                    for dj in range(pool_height):\n",
    "                        val = data[ic, x_out * pool_width + di, y_out * pool_height + dj]\n",
    "                        if val > max_value:\n",
    "                            max_value = 1 * val\n",
    "                            max_i, max_j = di, dj\n",
    "                ddata[ic, x_out * pool_width + max_i, y_out * pool_height + max_j] = output_grad[ic, x_out, y_out]          #oc = ic\n",
    "\n",
    "    return ddata\n",
    "    # *** END CODE HERE ***"
   ],
   "id": "728e3e9a04a66846"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def forward_cross_entropy_loss(probabilities, labels):\n",
    "    \"\"\"\n",
    "    Compute the output from a cross entropy loss layer given the probabilities and labels.\n",
    "\n",
    "    probabilities is of the shape (# classes)\n",
    "    labels is of the shape (# classes)\n",
    "\n",
    "    The output should be a scalar\n",
    "\n",
    "    Returns:\n",
    "        The result of the log loss layer\n",
    "    \"\"\"\n",
    "\n",
    "    result = 0\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        if label == 1:\n",
    "            result += -np.log(probabilities[i])\n",
    "\n",
    "    return result"
   ],
   "id": "c242d0b702c5a9af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def backward_cross_entropy_loss(probabilities, labels):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the cross entropy loss with respect to the probabilities.\n",
    "\n",
    "    probabilities is of the shape (# classes)\n",
    "    labels is of the shape (# classes)\n",
    "\n",
    "    The output should be the gradient with respect to the probabilities.\n",
    "\n",
    "    Returns:\n",
    "        The gradient of the loss with respect to the probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    # *** END CODE HERE ***"
   ],
   "id": "722b1f825c764a28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def forward_linear(weights, bias, data):\n",
    "    \"\"\"\n",
    "    Compute the output from a linear layer with the given weights, bias and data.\n",
    "    weights is of the shape (input # features, output # features)\n",
    "    bias is of the shape (output # features)\n",
    "    data is of the shape (input # features)\n",
    "\n",
    "    The output should be of the shape (output # features)\n",
    "\n",
    "    Returns:\n",
    "        The result of the linear layer\n",
    "    \"\"\"\n",
    "    return data.dot(weights) + bias"
   ],
   "id": "9f94740e7ed9370b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def backward_linear(weights, bias, data, output_grad):\n",
    "    \"\"\"\n",
    "    Compute the gradients of the loss with respect to the parameters of a linear layer.\n",
    "\n",
    "    See forward_linear for information about the shapes of the variables.\n",
    "\n",
    "    output_grad is the gradient of the loss with respect to the output of this layer.\n",
    "\n",
    "    This should return a tuple with three elements:\n",
    "    - The gradient of the loss with respect to the weights\n",
    "    - The gradient of the loss with respect to the bias\n",
    "    - The gradient of the loss with respect to the data\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    # *** END CODE HERE ***"
   ],
   "id": "e178a42eb20f4370"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def forward_prop(data, labels, params):\n",
    "    \"\"\"\n",
    "    Implement the forward layer given the data, labels, and params.\n",
    "\n",
    "    Args:\n",
    "        data: A numpy array containing the input (shape is 1 by 28 by 28)\n",
    "        labels: A 1d numpy array containing the labels (shape is 10)\n",
    "        params: A dictionary mapping parameter names to numpy arrays with the parameters.\n",
    "            This numpy array will contain W1, b1, W2 and b2\n",
    "            W1 and b1 represent the weights and bias for the hidden layer of the network\n",
    "            W2 and b2 represent the weights and bias for the output layer of the network\n",
    "\n",
    "    Returns:\n",
    "        A 2 element tuple containing:\n",
    "            1. A numpy array The output (after the softmax) of the output layer\n",
    "            2. The average loss for these data elements\n",
    "    \"\"\"\n",
    "\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "\n",
    "    first_convolution = forward_convolution(W1, b1, data)\n",
    "    first_max_pool = forward_max_pool(first_convolution, MAX_POOL_SIZE, MAX_POOL_SIZE)\n",
    "    first_after_relu = forward_relu(first_max_pool)\n",
    "\n",
    "    flattened = np.reshape(first_after_relu, (-1))\n",
    "\n",
    "    logits = forward_linear(W2, b2, flattened)\n",
    "\n",
    "    y = forward_softmax(logits)\n",
    "    cost = forward_cross_entropy_loss(y, labels)\n",
    "\n",
    "    return y, cost"
   ],
   "id": "b5b93f98e21eca71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def backward_prop(data, labels, params):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation gradient computation step for a neural network\n",
    "\n",
    "    Args:\n",
    "        data: A numpy array containing the input for a single example\n",
    "        labels: A 1d numpy array containing the labels for a single example\n",
    "        params: A dictionary mapping parameter names to numpy arrays with the parameters.\n",
    "            This numpy array will contain W1, b1, W2, and b2\n",
    "            W1 and b1 represent the weights and bias for the convolutional layer\n",
    "            W2 and b2 represent the weights and bias for the output layer of the network\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of strings to numpy arrays where each key represents the name of a weight\n",
    "        and the values represent the gradient of the loss with respect to that weight.\n",
    "\n",
    "        In particular, it should have 4 elements:\n",
    "            W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    # *** END CODE HERE ***"
   ],
   "id": "a05ffdbff5f759d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def forward_prop_batch(batch_data, batch_labels, params, forward_prop_func):\n",
    "    \"\"\"Apply the forward prop func to every image in a batch\"\"\"\n",
    "\n",
    "    y_array = []\n",
    "    cost_array = []\n",
    "\n",
    "    for item, label in zip(batch_data, batch_labels):\n",
    "        y, cost = forward_prop_func(item, label, params)\n",
    "        y_array.append(y)\n",
    "        cost_array.append(cost)\n",
    "\n",
    "    return np.array(y_array), np.array(cost_array)"
   ],
   "id": "3e56ee8e9eaac365"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def gradient_descent_batch(batch_data, batch_labels, learning_rate, params, backward_prop_func):\n",
    "    \"\"\"\n",
    "    Perform one batch of gradient descent on the given training data using the provided learning rate.\n",
    "\n",
    "    This code should update the parameters stored in params.\n",
    "    It should not return anything\n",
    "\n",
    "    Args:\n",
    "        batch_data: A numpy array containing the training data for the batch\n",
    "        train_labels: A numpy array containing the training labels for the batch\n",
    "        learning_rate: The learning rate\n",
    "        params: A dict of parameter names to parameter values that should be updated.\n",
    "        backward_prop_func: A function that follows the backwards_prop API\n",
    "\n",
    "    Returns: This function returns nothing.\n",
    "    \"\"\"\n",
    "\n",
    "    total_grad = {}\n",
    "\n",
    "    for i in range(batch_data.shape[0]):\n",
    "        grad = backward_prop_func(\n",
    "            batch_data[i, :, :],\n",
    "            batch_labels[i, :],\n",
    "            params)\n",
    "        for key, value in grad.items():\n",
    "            if key not in total_grad:\n",
    "                total_grad[key] = np.zeros(value.shape)\n",
    "\n",
    "            total_grad[key] += value\n",
    "\n",
    "    params['W1'] = params['W1'] - learning_rate * total_grad['W1']\n",
    "    params['W2'] = params['W2'] - learning_rate * total_grad['W2']\n",
    "    params['b1'] = params['b1'] - learning_rate * total_grad['b1']\n",
    "    params['b2'] = params['b2'] - learning_rate * total_grad['b2']\n",
    "\n",
    "    # This function does not return anything\n",
    "    return"
   ],
   "id": "cfc1ff9fb6435e14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "21bd04a1ac4fcded"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6b5e545fb980d250"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bdfceec213c624c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b26f357572febc51"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
